{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.3 Handling raster and vector data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous notebook, we worked through challenges that can be associated with working with larger-than-memory datasets. Here, we'll start where we left off, with a dataset that is organized in chronological order, and has a chunking strategy applied. \n",
    "\n",
    "So far, we've been looking at ice velocity data generated for vast spatial footprints. What we're actually interested in is how ice velocity varies within and across individual glaciers. To look at this more closely, we'll use glacier outlines the [Randolph Glacier Inventory](https://www.glims.org/rgi_user_guide/welcome.html) to subset the velocity data. Once we've done this, we can start to ask questions like how does velocity vary over time and across different seasons, where on the glacier do velocities tend to be fastest and most seasonally active, and how do these patterns vary across different glaciers? The last two notebooks of this tutorial will start to address these questions.\n",
    "\n",
    "In this notebook, we introduce vector data that describes spatial features that we're interested in. This notebook will:\n",
    "- Look at how to parse and inspect important geographic metadata.     \n",
    "- Project data to match the coordinate reference system (CRS) of another data object.    \n",
    "- Join raster and vector data by clipping the raster data cube by the spatial extent of a vector data frame.  \n",
    "- Write this clipped raster object to disk. \n",
    "\n",
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content.Section_A)=\n",
    "**[A. Read data using strategy identified in previous notebook](#a-read-data-using-strategy-identified-in-previous-notebook)**\n",
    "\n",
    "(content.Section_B)=\n",
    "**[B. Incorporate glacier outline (vector) data](#b-incorporate-glacier-outline-vector-data)**\n",
    "- 1) Read and reproject vector data\n",
    "- 2) Visualize spatial extents of glacier outlines and ITS_LIVE data cube\n",
    "- 3) Crop vector data to spatial extent of raster data\n",
    "\n",
    "(content.Section_C)=\n",
    "**[C. Combine raster and vector data](#c-combine-raster-and-vector-data)**\n",
    "- 1) Use vector data to crop raster data\n",
    "- 2) Write clipped raster data cube to disk\n",
    ":::\n",
    ":::{tab-item} Learning Goals\n",
    "#### Concepts\n",
    "- Understand how to parse and manage coordinate reference system metadata,  \n",
    "- Visualize vector dataframes with static and interactive plots,  \n",
    "- Spatial subsetting and clipping with vector and raster data,\n",
    "- Write raster data to disk.\n",
    "\n",
    "#### Techniques\n",
    "- Use [cf_xarray](https://cf-xarray.readthedocs.io/en/latest/) and [pyproj](https://pyproj4.github.io/pyproj/stable/) to access geographic metadata,\n",
    "- Perform spatial subset of vector dataframe with [GeoPandas](https://geopandas.org/en/stable/),\n",
    "- Clip raster data using vector data with [rioxarray](https://corteva.github.io/rioxarray/).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import cf_xarray\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import pyproj\n",
    "\n",
    "import itslive_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read data using strategy identified in previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be reading and writing files to disk, create a variable to hold the path to the root directory for this tutorial; we'll use this later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial1_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find url\n",
    "url = itslive_tools.find_granule_by_point([95.180191, 30.645973])\n",
    "# Read data cube without Dask\n",
    "dc = itslive_tools.read_in_s3(url, chunks=None)\n",
    "# Sort by mid-date\n",
    "dc = dc.sortby(\"mid_date\")\n",
    "# Visually check mid-date in chronological order\n",
    "dc.mid_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the last notebook, we want to chunk the dataset and to do that, we need to assign chunk sizes for each dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_dict = {\"mid_date\": 20000, \"y\": 10, \"x\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.chunk(chunking_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Incorporate glacier outline (vector) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read and reproject vector data\n",
    "\n",
    "As discussed in the [vector data](../../background/4_tutorial_data.md#vector-data) section of the 'Tutorial Data' page, the examples in this tutorial use glacier outlines from the Randolph Glacier Inventory], version 7 ([RGI7](https://www.glims.org/rgi_user_guide/welcome.html)). We'll specifically be looking at the 'South Asia East' region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_parquet(\"../data/vector_data/rgi7_region15_south_asia_east.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is **vital** to check the CRS, or Coordinate Reference Systems, when combining geospatial data from different sources. \n",
    "\n",
    "The RGI data are in the `EPSG:4326` CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRS information for the ITS_LIVE dataset is stored in the `mapping` array. An easy way to discover this is to use the `cf_xarray` package and search for the `grid_mapping` variable if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_crs = pyproj.CRS.from_cf(dc.mapping.attrs)\n",
    "cube_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the data is projected to UTM zone 46N (EPSG:32646).\n",
    "\n",
    "We choose to reproject the glacier outline to the CRS of the data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project rgi data to match itslive\n",
    "se_asia_prj = se_asia.to_crs(cube_crs)\n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many glaciers are represented in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(se_asia_prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Visualize spatial extents of glacier outlines and ITS_LIVE data cube\n",
    "\n",
    "In [Accessing S3 Data](1_accessing_itslive_s3_data.ipynb), we defined a function to create a vector object describing the footprint of a raster object; we'll use that again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector bbox of itslive\n",
    "bbox_dc = itslive_tools.get_bounds_polygon(dc)\n",
    "bbox_dc[\"geometry\"]\n",
    "# Check that all objects have correct crs\n",
    "assert dc.attrs[\"projection\"] == bbox_dc.crs == se_asia_prj.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outline of the itslive granule and the rgi dataframe together\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bbox_dc.plot(ax=ax, facecolor=\"None\", color=\"red\")\n",
    "se_asia_prj.plot(ax=ax, facecolor=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the glacier outlines in the dataset (in black) and the spatial extent of the ITS_LIVE velocity data cube (in red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Crop vector data to spatial extent of raster data\n",
    "\n",
    "The above plot shows the coverage of the vector dataset (`se_asia_prj`) in black, relative to the extent of the raster dataset (`bbox_dc`) in red. We use the [geopandas `.clip()`](https://geopandas.org/en/stable/docs/reference/api/geopandas.clip.html) method to subset the RGI polygons  to the footprint of the ITS_LIVE data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rgi to bounds\n",
    "se_asia_subset = gpd.clip(se_asia_prj, bbox_dc)\n",
    "se_asia_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `geopandas` `.explore()` method to interactively look at the RGI7 outlines contained within the ITS_LIVE granule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(max_lat=31, max_lon=95, min_lat=29, min_lon=97, location=[30.2, 95.5], zoom_start=8)\n",
    "\n",
    "bbox_dc.explore(\n",
    "    m=m,\n",
    "    style_kwds={\"fillColor\": \"None\", \"color\": \"red\"},\n",
    "    legend_kwds={\"labels\": [\"ITS_LIVE granule footprint\"]},\n",
    ")\n",
    "se_asia_subset.explore(m=m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above interactive map to select a glacier to look at in more detail below.\n",
    "\n",
    "However, notice that while the above code correctly produces a plot, it also throws a warning. We're going to ignore the warning for now, but if you're interested in a detailed example of how to trouble shoot and resolve this type of warning, check out the [appendix](../../endmatter/appendix.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write `bbox_dc` to file so that we can use it in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_dc.to_file(\"../data/vector_data/bbox_dc.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Combine raster and vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use vector data to crop raster data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to dig in and analyze this velocity dataset at smaller spatial scales, we first need to subset it. The following section and the next notebook ([Exploratory data analysis of a single glacier](4_exploratory_data_analysis_single.ipynb)) will focus on the spatial scale of a single glacier. \n",
    "\n",
    "The glacier we'll focus on is in the southeastern region of Tibetan Plateau in the Hengduan mountain range. It was selected in part because it is large enough that we should be able to observe velocity variability within the context of the spatial resolution of the ITS_LIVE dataset but is not so large that it is an extreme outlier for glaciers in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a  glacier to subset to\n",
    "single_glacier_vec = se_asia_subset.loc[se_asia_subset[\"rgi_id\"] == \"RGI2000-v7.0-G-15-16257\"]\n",
    "single_glacier_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it to file to that it can be used later\n",
    "single_glacier_vec.to_file(\"../data/single_glacier_vec.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the ITS_LIVE raster dataset has an assigned CRS attribute. We already know that the data is projected in the correct coordinate reference system (CRS), but the object may not be 'CRS-aware' yet (ie. have an attribute specifying its CRS). This is necessary for spatial operations such as clipping and reprojection. If `dc` doesn't have a CRS attribute, use `rio.write_crs()` to assign it. For more detail, see Rioxarray's [CRS Management documentation](https://corteva.github.io/rioxarray/stable/getting_started/crs_management.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.rio.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the subset vector data object and Rioxarray's [`.clip()` method](https://corteva.github.io/rioxarray/html/examples/clip_geom.html) to crop the data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "single_glacier_raster = dc.rio.clip(single_glacier_vec.geometry, single_glacier_vec.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Write clipped raster data cube to disk\n",
    " \n",
    "We want to use `single_glacier_raster` in the following notebook without going through all of the steps of creating it again. So, we write the object to file as a Zarr data cube so that we can easily read it into memory when we need it next. However, we'll see that there are a few steps we must go through before we can successfully write this object. \n",
    "\n",
    "\n",
    "We first re-chunk the `single_glacier_raster` into more optimal chunk sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster = single_glacier_raster.chunk({\"mid_date\": 20000, \"x\": 10, \"y\": 10})\n",
    "single_glacier_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to write `single_glacier_raster`, eg: \n",
    "\n",
    "```single_glacier_raster.to_zarr('data/itslive/glacier_itslive.zarr', mode='w')```\n",
    "\n",
    "We'll received an error related to encoding. \n",
    "\n",
    "The root cause is that the encoding recorded was appropriate for the source dataset, but is not valid anymore given all the transformations we have run up to this point. The easy solution here is to simply call `drop_encoding`. This will delete any existing encoding isntructions, and have Xarray automatically choose an encoding that will work well for the data. Optimizing the encoding of on-disk data is an advanced topic that we will not cover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to write the object as a Zarr group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster.drop_encoding().to_zarr(\"../data/raster_data/single_glacier_itslive.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we read a large object into memory and clipped it to the footprint of a single area of using a large vector dataframe, all the while managing coordinate reference system metadata of both objects. We then saved the clipped raster object to disk so that it can be easily reused. The next notebook demonstrates exploratory data analysis steps using the object we just wrote to disk. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
