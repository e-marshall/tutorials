{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{title_its_nb4}}\n",
    "\n",
    "{{intro}}\n",
    "In the previous notebook, we walked through initial steps to read and organize a large raster dataset, and to understand it in the context of spatial areas of interest represented by vector data. \n",
    "\n",
    "The previous notebook mainly focused on high-level processing operations:   \n",
    "\n",
    "1) Reading a multi-dimensional dataset,   \n",
    "2) Re-organizing along a given dimension,   \n",
    "3) Reducing a large raster object to smaller areas of interest represented by vector data, and   \n",
    "4) Strategies for completing these steps with larger-than-memory datasets.   \n",
    "\n",
    "In this notebook, we will continue performing initial data inspection and exploratory analysis but this time, focused on velocity data clipped to an individual glacier. We demonstrate Xarray functionality for common computations and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content:Section_A)=\n",
    "**[A. Data exploration](#a-data-exploration)**\n",
    "- {{a1_its_nb4}}\n",
    "- {{a2_its_nb4}}\n",
    "- {{a3_its_nb4}}\n",
    "\n",
    "(content:Section_B)=\n",
    "**[B. Comparing different satellites](#b-comparing-different-satellites)**\n",
    "- {{b1_its_nb4}}\n",
    "- {{b2_its_nb4}}\n",
    "\n",
    "(content:Section_C)=\n",
    "**[C. Examine velocity variability](#c-examine-velocity-variability)**\n",
    "- {{c1_its_nb4}}\n",
    "- {{c2_its_nb4}}\n",
    "- {{c3_its_nb4}}\n",
    "\n",
    "(content:section_D)=\n",
    "**[D. Computations  along time dimension](#d-computations--along-time-dimension)**\n",
    "- {{d1_its_nb4}}\n",
    "- {{d2_its_nb4}}\n",
    ":::\n",
    "\n",
    ":::{tab-item} Learning Goals\n",
    "\n",
    "{{concepts}}\n",
    "- Examining metadata, interpreting physical observable in the context of available metadata,\n",
    "- Sub-setting and visualizing raster datasets\n",
    "- 'Split-apply-combine' workflows,\n",
    "- Calculating summary statistics across given dimensions of a dataset,\n",
    "- Performing reductions across multi-dimensional datasets.\n",
    "\n",
    "{{techniques}}\n",
    "- Managing groups of Xarray objects with `Xarray.DataTree`,  \n",
    "- Grouped computations with `GroupBy`,  \n",
    "- Using matplotlib to visualize raster and vector data with satellite imagery basemaps,   \n",
    "- Combining Xarray with statistical packages like [SciPy](https://scipy.org/).\n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import warnings\n",
    "\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import xarray as xr\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the previous notebook, we will write some objects to disk. Create the same variable to store the path to the root directory for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial1_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{a1_its_nb4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raster\n",
    "single_glacier_raster = xr.open_zarr(\"../data/raster_data/single_glacier_itslive.zarr\", decode_coords=\"all\")\n",
    "# Read vector\n",
    "single_glacier_vector = gpd.read_file(\"../data/vector_data/single_glacier_vec.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(single_glacier_raster.satellite_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(single_glacier_raster.satellite_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cells show us that this dataset contains observations from Sentinel 1 & 2 and Landsat 4,5,6,7,8 & 9 satellite sensors. The dataset is 3.3 GB. \n",
    "\n",
    "Next, we want to perform computations that require us to load this object into memory. To do this, we use the [dask .compute()](https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.compute.html) method, which turns a 'lazy' object into an in-memory object. If you try to run compute on too large of an object, your computer may run out of RAM and the kernel being used in this python session will die (if this happens, click 'restart kernel' from the kernel drop down menu above).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster = single_glacier_raster.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you expand the data object to look at the variables, you will see that they no long hold `dask.array` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick sanity check, we'll convince ourselves that the clipping operation in the previous [notebook](initial_velocity_data_inspection.ipynb#A.-Crop-ITS_LIVE-granule-single-glacier-outline) worked correctly. We also show that we can plot both Xarray raster data and GeoPandas vector data overlaid on the same plot with a satellite image basemap as a background.\n",
    "\n",
    "A few notes about the following figure:\n",
    "- `single_glacier_raster` is a 3-dimensional object. We want to plot it in 2-d space with the RGI glacier outline. So, we perform a reduction (in this case, compute the mean), in order to reduce the dataset from 3-d to 2-d (Another option would be to select a single time step).\n",
    "- We could make this plot with a white background, but it is also nice to be able to add a base map to the image. Here, we'll use [contextily](https://contextily.readthedocs.io/en/latest/) to do so. This will require converting the coordinate reference system (CRS) of both objects to the Web Mercator projection (EPSG:3857)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that CRS of vector and raster data are the same\n",
    "assert single_glacier_raster.rio.crs == single_glacier_vector.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster = single_glacier_raster.rio.write_crs(single_glacier_raster.attrs[\"projection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that CRS of vector and raster data are the same\n",
    "assert single_glacier_raster.rio.crs == single_glacier_vector.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject both objects to web mercator\n",
    "single_glacier_vector_web = single_glacier_vector.to_crs(\"EPSG:3857\")\n",
    "single_glacier_raster_web = single_glacier_raster.rio.reproject(\"EPSG:3857\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot objects\n",
    "single_glacier_raster_web.v.mean(dim=\"mid_date\").plot(ax=ax, cmap=\"viridis\", alpha=0.75, add_colorbar=True)\n",
    "single_glacier_vector_web.plot(ax=ax, facecolor=\"None\", edgecolor=\"red\", alpha=0.75)\n",
    "# Add basemap\n",
    "cx.add_basemap(ax, crs=single_glacier_vector_web.crs, source=cx.providers.Esri.WorldImagery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sorted along the time dimension in the previous notebook, so it should be in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster.mid_date[:6].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{a2_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide variety of forces can impact both satellite imagery and the ability of ITS_LIVE's feature tracking algorithm to extract velocity estimates from satellite image pairs. For these reasons, there are at times both gaps in coverage and ranges in the estimated error associated with different observations. The following section will demonstrate how to calculate and visualize coverage of the dataset over time. Part 2 will include a discussion of uncertainty and error estimates\n",
    "\n",
    "When first investigating a dataset, it is helpful to be able to scan/quickly visualize coverage along a given dimension. To create the data needed for such a visualization, we first need a mask that will tell us all possible 'valid' pixels; in other words, we need to differentiate between pixels in our 2-d rectangular array that represent ice v. non-ice. Then, for every time step, we can calculate the portion of possible ice pixels that contain an estimated velocity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of valid pixels\n",
    "valid_pixels = single_glacier_raster.v.count(dim=[\"x\", \"y\"])\n",
    "# calculate max. number of valid pixels\n",
    "valid_pixels_max = single_glacier_raster.v.notnull().any(\"mid_date\").sum([\"x\", \"y\"])\n",
    "# add cov proportion to dataset as variable\n",
    "single_glacier_raster[\"cov\"] = valid_pixels / valid_pixels_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize coverage over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "# Plot object\n",
    "single_glacier_raster[\"cov\"].plot(ax=ax, linestyle=\"None\", marker=\"x\", alpha=0.75)\n",
    "\n",
    "# Specify axes labels and title\n",
    "fig.suptitle(\"Velocity data coverage ovver time\", fontsize=16)\n",
    "ax.set_ylabel(\"Coverage (proportion)\", x=-0.05, fontsize=12)\n",
    "ax.set_xlabel(\"Date\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{a3_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have a dense time series of velocity observations for a given glacier (~48,000 observations from 1986-2024). However, we know that the ability of satellite imagery pairs to capture ice displacement (and by extension, velocity), can be impacted by conditions such as cloud-cover, which obscure Earth's surface from optical sensors. ITS_LIVE is a multi-sensor ice velocity dataset, meaning that it is composed of ice velocity observations derived from a number of satellites, which include both optical and Synthetic Aperture Radar (SAR) imagery. Currently, Sentinel-1 is the only SAR sensor included in ITS_LIVE, all others are optical. \n",
    "\n",
    "While optical imagery requires solar illumination and can be impacted by cloud cover, Sentinel-1 is an active remote sensing technique and images in a longer wavelength (C-band, ~ 5 cm). This means that Sentinel-1 imagery does not require solar illumination, and can penetrate through cloud cover. Because of these sensors differing sensitivity to Earth's surface conditions, there can sometimes be discrepancies in velocity data observed from different sensors. \n",
    "\n",
    "```{note}\n",
    "There are many great resources available for understanding the principles of SAR and working with SAR imagery. Check out the SAR section of the [tutorial data](../../background/4_tutorial_data.md) for more detail and links to additional resources.\n",
    "```\n",
    "\n",
    "Let's first look at what sensors are represented in the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = np.unique(single_glacier_raster.satellite_img1.values).tolist()\n",
    "sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract observations from a single satellite sensor, we will use Xarray indexing and selection methods such as `.where()` and `.sel()`. The following cells demonstrate different selection approaches and a brief discussion of the pros and cons of each when working with large and/ or sparse datasets.\n",
    "\n",
    "#### Landsat 8\n",
    "First, looking at velocity observations from Landsat8 data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l8_data = single_glacier_raster.where(single_glacier_raster[\"satellite_img1\"] == \"8\", drop=True)\n",
    "l8_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach: using `.sel()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l8_condition = single_glacier_raster.satellite_img1 == \"8\"\n",
    "l8_data_alt = single_glacier_raster.sel(mid_date=l8_condition)\n",
    "l8_data_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach using `.sel()` takes much less time than `.where()`. This is because `.sel()` queries the dataset using `mid_date` index. Xarray dimensions have associated `Index` objects, which are built on Pandas Indexes. They are very powerful for quickly and efficiently querying large datasets. In contrast, `.where()` must check each `satellite_img` data variable against the specified condition ('8'), which is not as efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about a sensor with multiple identifiers?\n",
    "\n",
    "For Landsat8 observations, we only needed to identify elements of the dataset where the `satellite_img1` variable matched a unique identifier, '8'. Sentinel-1 is a two-satellite constellation, meaning that it has sensors on board multiple satellites. For this, we need to select all observations where `satellite_img1` matches a list of possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s1_condition = single_glacier_raster.satellite_img1.isin([\"1A\", \"1B\"])\n",
    "s1_data = single_glacier_raster.sel(mid_date=s1_condition)\n",
    "s1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Comparing different satellites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to use Xarray's labeled dimensions and indexing and selection capabilities to subset the dataset based on its metadata. What if we wanted to use those subsets, for example to visualize data or perform computations for each subset group? We could do this manually, but it would be quite time-consuming and involve repeating the same steps for each group. Luckily, there are a few different ways that we can approach this without writing a lot of duplicate code. \n",
    "\n",
    "Below, we show two different approaches to split the dataset into subset based on a metadata condition, perform a calculation on each subset and then visualize the results for each subset side-by-side. \n",
    "\n",
    "Start by making a dict of each sensor and its identifying string(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_conditions = {\n",
    "    \"Landsat 4\": [\"4\"],\n",
    "    \"Landsat 5\": [\"5\"],\n",
    "    \"Landsat 7\": [\"7\"],\n",
    "    \"Landsat 8\": [\"8\"],\n",
    "    \"Landsat 9\": [\"9\"],\n",
    "    \"Sentinel 1\": [\"1A\", \"1B\"],\n",
    "    \"Sentinel 2\": [\"2A\", \"2B\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{b1_its_nb4}}\n",
    "\n",
    "Rather than go through the above steps for each sensor, we write a function that subsets the dataset by sensor, returning a dict of Xarray datasets holding velocity time series for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_ds_by_sensor(ds, sensor_conditions):\n",
    "    # Make empty lists to hold sensor IDs and subsetted datasets\n",
    "    keys_ls, vals_ls = [], []\n",
    "\n",
    "    # Iterate through each sensor in dict\n",
    "    for sensor in sensor_conditions.keys():\n",
    "        condition = ds.satellite_img1.isin(sensor_conditions[sensor])\n",
    "\n",
    "        # Use .sel to subset data based on sensor\n",
    "        sensor_data = ds.sel(mid_date=condition)\n",
    "\n",
    "        keys_ls.append(f\"{sensor}\")\n",
    "        vals_ls.append(sensor_data)\n",
    "\n",
    "    # Return dict of sensor IDs and subsetted datasets\n",
    "    ds_dict = dict(zip(keys_ls, vals_ls))\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds_dict = separate_ds_by_sensor(single_glacier_raster, sensor_conditions)\n",
    "print(sensor_ds_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at one subset to see that it matches the one we made manually above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds_dict[\"Landsat 8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've separated the dataset into subset, we'd like a way to work with those subsets as a group, rather than applying operations to each one. For this, we can use a new structure within the Xarray data model, `xr.DataTree`, which facilitates working with collections of Xarray data objects. \n",
    "\n",
    "The use-case in this section is that we would like to efficiently compute the mean along the time dimension of the ITS_LIVE dataset for each satellite sensor, and then visualize the results.\n",
    "\n",
    "Before the implementation of Xarray DataTree, we would need to either perform the sequence of operations on each sensor-specific dataset individually, or, create a dict of the sensor specific datasets, write a function to perform certain operations and either update the dictionary or create another to hold the results; both of these options are quite clunky and inefficient. \n",
    "\n",
    "We can create an `xr.DataTree` object by using the `from_dict()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds_tree = xr.DataTree.from_dict(sensor_ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataTree has a parent node and groups (shown above) that contain individual `xr.Datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ds_tree[\"Landsat 8\"].ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to perform a set of operations on every node of the Datatree, we can define a function to pass to `xr.DataTree.map_over_datasets()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temporal_mean(ds):\n",
    "    \"\"\"I'm a function that calculates the temporal mean of a dataset with (x,y,mid_date) dimensions.\n",
    "    I return a new dataset with (x,y,sensor) dimensions\"\"\"\n",
    "\n",
    "    # Skip parent node\n",
    "    if len(ds.data_vars) == 0:\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        # Calc mean\n",
    "        tmean = ds.mean(dim=\"mid_date\")\n",
    "        # Add a sensor dimension -- this will be used for combining them back together later\n",
    "        sensor = np.unique(ds.satellite_img1.data)\n",
    "        if len(sensor) > 1:\n",
    "            sensor = [sensor[0]]\n",
    "        # Expand dims to add sensor\n",
    "        tmean = tmean.expand_dims({\"sensor\": sensor})\n",
    "        return tmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mean_tree = sensor_ds_tree.map_over_datasets(calc_temporal_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take just the descendent nodes of this Datatree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_tree = temp_mean_tree.descendants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and concatenate them into a single `xr.Dataset` along the `'sensor'` dimension we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_mean_ds = xr.concat([child_tree[i].ds for i in range(len(child_tree))], dim=\"sensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_mean_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use Xarray's `FacetGrid` plotting to visualize them side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_v_magnitude(ds):\n",
    "    \"\"\"I'm a function that calculates the magnitude of a velocity displacement vector given velocity component vectors.\n",
    "    I return the same dataset object with a new variable called 'vmag'\"\"\"\n",
    "\n",
    "    ds[\"vmag\"] = np.sqrt(ds[\"vx\"] ** 2 + ds[\"vy\"] ** 2)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# First, calculate magnitude of velocity from the temporal mean of the component vectors\n",
    "sensor_mean_ds = calc_v_magnitude(sensor_mean_ds)\n",
    "\n",
    "a = sensor_mean_ds[\"vmag\"].plot(col=\"sensor\", col_wrap=4, cbar_kwargs={\"label\": \"Meters / year\"})\n",
    "a.fig.suptitle(\"Temporal mean of velocity magnitude\", fontsize=16, y=1.05)\n",
    "a.fig.supylabel(\"Y-coordinate of projection (meters)\", fontsize=12, x=-0.02)\n",
    "a.fig.supxlabel(\"X-coordinate of projection (meters)\", fontsize=12, y=-0.05)\n",
    "\n",
    "# remove individual axes labesl\n",
    "for i in range(len(a.axs[0])):\n",
    "    a.axs[0][i].set_ylabel(None)\n",
    "    a.axs[0][i].set_xlabel(None)\n",
    "\n",
    "for i in range(len(a.axs[1])):\n",
    "    a.axs[1][i].set_ylabel(None)\n",
    "    a.axs[1][i].set_xlabel(None)\n",
    "    a.axs[1][i].tick_params(axis=\"x\", labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to keep in mind that in addition to having different spectral properties and imaging resolutions, the sensors included in the ITS_LIVE dataset have been active during different, and sometimes overlapping periods of time. There is additional discussion of inter-sensor bias in the ITS_LIVE Known Issues [documentation](http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/documentation/ITS_LIVE-Regional-Glacier-and-Ice-Sheet-Surface-Velocities-Known-Issues.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{b2_its_nb4}}\n",
    "\n",
    "\n",
    "The previous section is a great example of a split-apply-combine workflow, where we split the dataset up in to multiple datasets by sensor, applied the `mean` computation, and then combined the results back. This is the GroupBy paradigm.\n",
    "\n",
    "An alternate approach to the above figure would be to first use the `sensor_conditions` mapping to create a variable to group by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = single_glacier_raster.satellite_img1\n",
    "sensor.name = \"sensor\"\n",
    "for label, values in sensor_conditions.items():\n",
    "    sensor = xr.where(sensor.isin(values), label, sensor)\n",
    "sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now group the data by sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = single_glacier_raster.groupby(sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mean reduction to each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_means = grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a 3D Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make the same figure as earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_means = calc_v_magnitude(sensor_means)\n",
    "\n",
    "a = sensor_means[\"vmag\"].plot(col=\"sensor\", col_wrap=4, cbar_kwargs={\"label\": \"Meters / year\"})\n",
    "a.fig.suptitle(\"Temporal mean of velocity magnitude\", fontsize=16, y=1.05)\n",
    "a.fig.supylabel(\"Y-coordinate of projection (meters)\", fontsize=12, x=-0.02)\n",
    "a.fig.supxlabel(\"X-coordinate of projection (meters)\", fontsize=12, y=-0.05)\n",
    "\n",
    "# remove individual axes labesl\n",
    "for i in range(len(a.axs[0])):\n",
    "    a.axs[0][i].set_ylabel(None)\n",
    "    a.axs[0][i].set_xlabel(None)\n",
    "\n",
    "for i in range(len(a.axs[1])):\n",
    "    a.axs[1][i].set_ylabel(None)\n",
    "    a.axs[1][i].set_xlabel(None)\n",
    "    a.axs[1][i].tick_params(axis=\"x\", labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing an approach\n",
    "\n",
    "The two paradigms, hierarchical Datatree and groupby, are equivalent ways of solving the problem. Choose the approach that makes most sense for your analysis. In this case our analysis is a simple operation (`mean`) and the result is a regular 3D array, so the groupby approach is less complex. For more complicated datasets, for example those where each sensor is a separate cube on a different grid, the Datatree approach is more appropriate and ergonomic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Examine velocity variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{c1_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we plot histogram of the `v`, `vx` and `vy` variables to examine their distributions. To construct these plots, we use a combination of xarray plotting functionality and matplotlib object-oriented plotting. In addition, we use [`xr.reduce()`](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.reduce.html) and [`scipy.stats.skew()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html) to calculate the skew of each variable (inset in each sub-plot).\n",
    "\n",
    "To make things easier, write a function that calculates and hold summary statistics for each variable in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summary_stats(ds: xr.Dataset, variable: str):\n",
    "    \"\"\"I'm a function that calculates summary statistics for a given data variable and returns them as a dict to be used in a plot\"\"\"\n",
    "\n",
    "    skew = ds[f\"{variable}\"].reduce(func=scipy.stats.skew, nan_policy=\"omit\", dim=[\"x\", \"y\", \"mid_date\"]).data\n",
    "    mean = ds[f\"{variable}\"].mean(dim=[\"x\", \"y\", \"mid_date\"], skipna=True).data\n",
    "    median = ds[f\"{variable}\"].median(dim=[\"x\", \"y\", \"mid_date\"], skipna=True).data\n",
    "\n",
    "    stats_dict = {\"skew\": skew, \"mean\": mean, \"median\": median}\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_vy = calc_summary_stats(single_glacier_raster, \"vy\")\n",
    "stats_vx = calc_summary_stats(single_glacier_raster, \"vx\")\n",
    "stats_v = calc_summary_stats(single_glacier_raster, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "# VY\n",
    "hist_y = single_glacier_raster.vy.plot.hist(ax=axs[0], bins=100)\n",
    "cumulative_y = np.cumsum(hist_y[0])\n",
    "axs[0].plot(hist_y[1][1:], cumulative_y, color=\"orange\", linestyle=\"-\", alpha=0.5)\n",
    "# VY stats text\n",
    "axs[0].text(x=-2000, y=2e6, s=f\"Skew: {stats_vy['skew']:.3f}\", fontsize=12, color=\"black\")\n",
    "axs[0].text(x=-2000, y=1.5e6, s=f\"Mean: {stats_vy['mean']:.3f}\", fontsize=12, color=\"black\")\n",
    "axs[0].text(x=-2000, y=1e6, s=f\"Median: {stats_vy['median']:.3f}\", fontsize=12, color=\"black\")\n",
    "\n",
    "# VX\n",
    "hist_x = single_glacier_raster.vx.plot.hist(ax=axs[1], bins=100)\n",
    "cumulative_x = np.cumsum(hist_x[0])\n",
    "axs[1].plot(hist_x[1][1:], cumulative_x, color=\"orange\", linestyle=\"-\", alpha=0.5)\n",
    "# VX stats text\n",
    "axs[1].text(x=-2000, y=2e6, s=f\"Skew: {stats_vx['skew']:.3f}\", fontsize=12, color=\"black\")\n",
    "axs[1].text(x=-2000, y=1.5e6, s=f\"Mean: {stats_vx['mean']:.3f}\", fontsize=12, color=\"black\")\n",
    "axs[1].text(x=-2000, y=1e6, s=f\"Median: {stats_vx['median']:.3f}\", fontsize=12, color=\"black\")\n",
    "\n",
    "# V\n",
    "hist_v = single_glacier_raster.v.plot.hist(ax=axs[2], bins=100)\n",
    "cumulative_v = np.cumsum(hist_v[0])\n",
    "axs[2].plot(hist_v[1][1:], cumulative_v, color=\"orange\", linestyle=\"-\", alpha=0.5)\n",
    "# V stats text\n",
    "axs[2].text(x=2000, y=2e6, s=f\"Skew: {stats_v['skew']:.3f}\", fontsize=12, color=\"black\")\n",
    "axs[2].text(x=2000, y=1.5e6, s=f\"Mean: {stats_v['mean']:.3f}\", fontsize=12, color=\"black\")\n",
    "axs[2].text(x=2000, y=1e6, s=f\"Median: {stats_v['median']:.3f}\", fontsize=12, color=\"black\")\n",
    "\n",
    "# Formatting and labeling\n",
    "axs[0].set_title(\"VY\")\n",
    "axs[1].set_title(\"VX\")\n",
    "axs[2].set_title(\"V)\")\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_xlabel(None)\n",
    "    axs[i].set_ylabel(None)\n",
    "\n",
    "fig.supylabel(\"# Observations\", x=0.08, fontsize=12)\n",
    "fig.supxlabel(\"Meters / year\", fontsize=12)\n",
    "fig.suptitle(\n",
    "    \"Histogram (blue) and cumulative distribution function (orange) of velocity components and magnitude\",\n",
    "    fontsize=16,\n",
    "    y=1.05,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and summary statistics show that `vx` and `vy` distributions are relatively Gaussian, while `v` is positively skewed and [Rician](https://en.wikipedia.org/wiki/Rice_distribution). This is due to the non-linear relationship between component and displacement vectors. In datasets such as this one where the signal to noise ratio can be low, calculating velocity magnitude on smoothed or averaged component vectors can help to suppress noise (for a bit more detail, refer to this [comment](https://github.com/e-marshall/itslive/issues/26)). For this reason, we will usually calculate velocity magnitude after the dataset has been reduced over space or time dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{c2_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a greater understanding of the importance of velocity component variability in shaping our understandings of velocity variability, let's examine these variables as well as the estimated error provided in the dataset by reducing the dataset along the temporal dimension so that we can visualize the data along x and y dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min, max for color bar\n",
    "vmin_y = single_glacier_raster.vy.mean(dim=[\"mid_date\"]).min().data\n",
    "vmax_y = single_glacier_raster.vy.mean(dim=[\"mid_date\"]).max().data\n",
    "\n",
    "vmin_x = single_glacier_raster.vx.mean(dim=[\"mid_date\"]).min().data\n",
    "vmax_x = single_glacier_raster.vx.mean(dim=[\"mid_date\"]).max().data\n",
    "\n",
    "vmin = min([vmin_x, vmin_y])\n",
    "vmax = max([vmax_x, vmax_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(17, 7))\n",
    "\n",
    "x = single_glacier_raster.vx.mean(dim=\"mid_date\").plot(ax=axs[0], vmin=vmin, vmax=vmax, cmap=\"RdBu_r\")\n",
    "y = single_glacier_raster.vy.mean(dim=\"mid_date\").plot(ax=axs[1], vmin=vmin, vmax=vmax, cmap=\"RdBu_r\")\n",
    "axs[0].set_title(\"x-component velocity\", fontsize=12)\n",
    "axs[1].set_title(\"y-component velocity\", fontsize=12)\n",
    "fig.suptitle(\"Temporal mean of velocity components\", fontsize=16, y=1.02)\n",
    "\n",
    "x.colorbar.set_label(\"m/y\", rotation=270)\n",
    "y.colorbar.set_label(\"m/yr\", rotation=270)\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_ylabel(None)\n",
    "    axs[i].set_xlabel(None)\n",
    "fig.supylabel(\"Y-coordinate of projection (meters)\", x=0.08, fontsize=12)\n",
    "fig.supxlabel(\"X-coordinate of projection (meters)\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to visualizing components (above), plotting velocity vectors is helpful for understanding magnitude and direction of flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate and visualize mean velocity magnitude over time (we will use the function defined in Part 1), and the mean estimated error over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_v = calc_v_magnitude(single_glacier_raster.mean(dim=\"mid_date\", skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 7))\n",
    "\n",
    "single_glacier_vector.plot(ax=axs[0], facecolor=\"none\", edgecolor=\"red\")\n",
    "single_glacier_raster.mean(dim=\"mid_date\").plot.quiver(\"x\", \"y\", \"vx\", \"vy\", ax=axs[1], angles=\"xy\", robust=True)\n",
    "\n",
    "single_glacier_vector.plot(ax=axs[1], facecolor=\"none\", edgecolor=\"red\")\n",
    "a = ds_v[\"vmag\"].plot(ax=axs[0], alpha=0.6, vmax=45, vmin=5)\n",
    "a.colorbar.set_label(\"meter/year\")\n",
    "\n",
    "fig.supylabel(\"Y-coordinate of projection (meters)\", x=0.08, fontsize=12)\n",
    "fig.supxlabel(\"X-coordinate of projection (meters)\", fontsize=12)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Velocity vectors (R) and magntiude of velocity (L), averaged over time\",\n",
    "    fontsize=16,\n",
    "    y=0.98,\n",
    ")\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_xlabel(None)\n",
    "    axs[i].set_ylabel(None)\n",
    "    axs[i].set_title(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize magnitude of velocity overlaid with velocity vectors next to velocity error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(22, 6), ncols=2)\n",
    "\n",
    "vmag = ds_v.vmag.plot(ax=ax[0], vmin=0, vmax=52, alpha=0.5)\n",
    "single_glacier_raster.mean(dim=\"mid_date\").plot.quiver(\"x\", \"y\", \"vx\", \"vy\", ax=ax[0], angles=\"xy\", robust=True)\n",
    "\n",
    "err = ds_v.v_error.plot(ax=ax[1], vmin=0, vmax=52)\n",
    "\n",
    "\n",
    "vmag.colorbar.set_label(\"m/y\")  # , rotation=270)\n",
    "err.colorbar.set_label(\"m/y\")  # , rotation=270)\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_ylabel(None)\n",
    "    ax[i].set_xlabel(None)\n",
    "    ax[i].set_title(None)\n",
    "\n",
    "fig.supxlabel(\"X-coordinate of projection (meters)\", fontsize=12)\n",
    "fig.supylabel(\"Y-coordinate of projection (meters)\", x=0.08, fontsize=12)\n",
    "fig.suptitle(\n",
    "    \"Mean velocity magnitude over time (L), mean error over time (R)\",\n",
    "    fontsize=16,\n",
    "    y=1.02,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`v_error` is large relative to the magnitude of velocity, suggesting that this data is pretty noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{c3_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce over the spatial dimensions (this time we will switch it up and choose a different reduction function) and visualize variability over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "vmag_med = calc_v_magnitude(single_glacier_raster.median(dim=[\"x\", \"y\"]))\n",
    "\n",
    "vmag_med.plot.scatter(x=\"mid_date\", y=\"vmag\", ax=ax, marker=\"o\", edgecolors=\"None\", alpha=0.5)\n",
    "fig.suptitle(\"Spatial median magnitude of velocity over time\")\n",
    "ax.set_title(None)\n",
    "ax.set_ylabel(\"m/y\")\n",
    "ax.set_xlabel(\"Time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps get a sense of velocity variability over time, but also shows how many outliers there are, even after taking the median over the x and y dimensions. In the final section of this notebook, we explore different approaches for changing the resolution of the temporal dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Computations  along time dimension\n",
    "\n",
    "In Part 2, we saw that the timeseries is dense in places and quite noisy. This section demonstrates different approaches for looking at a temporal signal in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{d1_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Xarray's [`resample()`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.resample.html) method to coarsen the temporal resolution of the dataset. With `resample()`, you can upsample or downsample the data, choosing both the resample frequency and the type of reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coarsen dataset to monthly\n",
    "resample_obj = single_glacier_raster.resample(mid_date=\"2ME\")\n",
    "# Calculate monthly median\n",
    "glacier_resample_1mo = resample_obj.median(dim=\"mid_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mid_date` dimension is now much less dense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_resample_1mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 2-month resampled median time series to the full time series we plotted at the end of Part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "# Calculate magnitude of velocity after the temporal reduction\n",
    "vmag_1mo = calc_v_magnitude(glacier_resample_1mo)\n",
    "\n",
    "# Calculate spatial median\n",
    "vmag_1mo[\"vmag\"].median(dim=[\"x\", \"y\"]).plot(ax=ax)\n",
    "# Plot full time series (spatial median) magnitude of velocity\n",
    "vmag_med.plot.scatter(\n",
    "    x=\"mid_date\",\n",
    "    y=\"vmag\",\n",
    "    ax=ax,\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"None\",\n",
    "    alpha=0.5,\n",
    "    color=\"orange\",\n",
    ")\n",
    "# Labels and formatting\n",
    "fig.suptitle(\n",
    "    \"2-month median magnitude of velocity (blue), full time series (orange)\",\n",
    "    fontsize=16,\n",
    ")\n",
    "ax.set_title(None)\n",
    "ax.set_ylabel(\"m/y\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylim(0, 750);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a few observations:\n",
    "- Computing two-month median velocities makes it easier to see a somewhat periodic velocity signal\n",
    "- As expected, the median is highly sensitive to the density of observations; as such, it displays greater amplitude during periods with sparser observations. When the density of observations increases significantly in 2014, the amplitude of variability of the 2-month median curve decreases dramatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{d2_its_nb4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray's [`groupby()`](https://docs.xarray.dev/en/stable/user-guide/groupby.html) functionality allows us to segment the dataset into different groups along given dimensions. Here, we use that to analyze seasonal velocity variability patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_gb = single_glacier_raster.groupby(single_glacier_raster.mid_date.dt.season).median()\n",
    "# add attrs to gb object\n",
    "seasons_gb.attrs = single_glacier_raster.attrs\n",
    "# Reorder seasons\n",
    "seasons_gb = seasons_gb.reindex({\"season\": [\"DJF\", \"MAM\", \"JJA\", \"SON\"]})\n",
    "seasons_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cool; we've gone from our 3-d object with a very dense `mid_date` dimension to a 3-d object where the temporal aspect of the data is represented by 4 seasons. \n",
    "\n",
    "In the above cell, we defined how we wanted to group our data (`single_glacier_raster.mid_date.dt.season`) and the reduction we wanted to apply to each group (`median()`). After the apply step, Xarray automatically combines the groups into a single object with a `season` dimension.\n",
    "\n",
    " If you'd like to see another example of this with more detailed explanations, go [here](https://tutorial.xarray.dev/fundamentals/03.2_groupby_with_xarray.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate magnitude on seasonal groupby object\n",
    "seasons_vmag = calc_v_magnitude(seasons_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use another `FacetGrid` plot to visualize the seasons side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = seasons_vmag.vmag.plot(col=\"season\", cbar_kwargs={\"label\": \"Meters / year\"})\n",
    "fg.fig.suptitle(\"Seasonal median velocity magnitude\", fontsize=16, y=1.05)\n",
    "fg.fig.supxlabel(\"X-coordinate of projection (meters)\", fontsize=12, y=-0.05)\n",
    "fg.fig.supylabel(\"Y-coordinate of projection (meters)\", fontsize=12, x=-0.02)\n",
    "for i in range(len(fg.axs[0])):\n",
    "    fg.axs[0][i].set_ylabel(None)\n",
    "    fg.axs[0][i].set_xlabel(None)\n",
    "    fg.axs[0][i].tick_params(axis=\"x\", labelrotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above FacetGrid plot, it appears that some regions of the glacier are very active (show high velocities) throughout the entire year. In other areas, it appears that glacier flow may be much more seasonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{conclusion}}  \n",
    "This was a primer in exploratory data analysis at the scale of an individual spatial area of interest (in this case, a glacier). The last notebook in this chapter will demonstrate exploratory analysis at a larger spatial scale."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
