{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cbba21-1acd-42df-8642-d03abe72c7b2",
   "metadata": {},
   "source": [
    "# 4.3 Exploratory analysis of ASF S1 imagery\n",
    "\n",
    "Now that we have read in and organized the stack of Sentinel-1 RTC images, let's take a look at the data.\n",
    "\n",
    ":::{admonition} ASF data access options\n",
    "The steps shown in this notebook involve downloading and extracting large volumes of data. **It is not necessary to do this to follow the rest of the content in the tutorial**. We include the demonstration for the purposes of completeness and to help users who may be in this situation.\n",
    "\n",
    "For more information on different options for downloading data locally, see the [Introduction](../s1_intro.md#different-ways-to-use-this-tutorial).\n",
    ":::\n",
    "\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content.section_A)=\n",
    "**[A. Read and prepare data](#a-read-and-prepare-data)**  \n",
    "- 1) Clip to spatial area of interest  \n",
    "\n",
    "(content.section_B)=\n",
    "**[B. Layover-shadow map](#b-layover-shadow-map)**  \n",
    "- 1) Interactive visualization of layover-shadow maps\n",
    "\n",
    "(content.section_C)=\n",
    "**[C. Orbital direction](#c-orbital-direction)**  \n",
    "- 1) Is a pass ascending or descending?\n",
    "- 2) Assign orbital direction as a coordinate variable\n",
    "\n",
    "(content.section_D)=\n",
    "**[D. Duplicate time steps](#d-duplicate-time-steps)**  \n",
    "- 1) Identify duplicate time steps\n",
    "- 2) Visualize duplicates\n",
    "- 3) Drop duplicates\n",
    "\n",
    "(content.section_E)=\n",
    "**[E. Examine coverage over time series](#e-examine-coverage-over-time-series)**\n",
    "- 1) Calculate coverage\n",
    "\n",
    "(content.section_F)=\n",
    "**[F. Data Visualization](#f-data-visualization)**\n",
    "- 1) Mean backscatter over time\n",
    "- 2) Seasonal backscatter variability\n",
    "- 3) Backscatter time series\n",
    "\n",
    ":::\n",
    ":::{tab-item} Learning goals  \n",
    "#### Concepts\n",
    "- Spatial joins of raster and vector data.  \n",
    "- Visualize raster data.  \n",
    "- Use raster metadata to aid interpretation of backscatter imagery.  \n",
    "- Examine data quality using provided layover-shadow maps.  \n",
    "- Identify and remove duplicate time step observations.  \n",
    "\n",
    "#### Techniques\n",
    "- Clip raster data cube using vector data with [`rioxarray.clip()`](https://corteva.github.io/rioxarray/html/examples/clip_geom.html).  \n",
    "- Using `xr.groupby()` for [grouped statistics](https://docs.xarray.dev/en/stable/user-guide/groupby.html).  \n",
    "- Reorganizing data with `xr.Dataset.reindex()`.  \n",
    "- Visualizing multiple facets of the data using `FacetGrid`\n",
    "\n",
    "\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::{admonition} ASF Data Access\n",
    "You can download the RTC-processed backscatter time series [here](https://zenodo.org/record/7236413#.Y1rNi37MJ-0). For more detail, see [tutorial data](../../background/4_tutorial_data.md#sentinel-1-rtc-datasets) and the [notebook](1_read_asf_data.ipynb) on reading ASF Sentinel-1 RTC data into memory.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f4a3f-1adc-4f61-979a-94ec5de93f38",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# %xmode minimal\n",
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "import s1_tools\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ae6a2",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3840e-f4a6-4bb3-ad23-1e83a0e5aa0b",
   "metadata": {},
   "source": [
    "## A. Read and prepare data\n",
    "\n",
    "We'll go through the steps shown in [metadata wrangling](2_wrangle_metadata.ipynb), but this time,  combined into one function from `s1_tools`. \n",
    "\n",
    ":::{attention} \n",
    "If you are following along on your own computer, you **must** specify `'timeseries_type'` below: \n",
    "1. Set `timeseries_type` to `'full'` or `'subset'` depending on if you are using the full time series (103 files) or the subset time series (5 files).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time series type\n",
    "timeseries_type = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e705893",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The default location of the data downloaded in this tutorial (in the [data download notebook](download_zenodo_data_curl.ipynb) is:  \n",
    "`../book/sentinel1/data/raster_data/{timeseries_type}_timeseries/'`.   \n",
    "Here, `{timeseries_type} is a stand-in for 'full' or 'subset', depending on which data you're using. \n",
    "\n",
    "If you prefer to store the data in a different location on your computer, update the `path_to_rtcs` variable below to be the full path of the downloaded data. The path should end with 'asf_rtcs.' For example, if you plan to store the data for this tutorial in your Downloads directory, you should specify `path_to_rtcs` as:   \n",
    "`path_to_rtcs = \"/home/User/Downloads/asf_rtcs\"`\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to RTCs\n",
    "path_to_rtcs = f\"../data/raster_data/{timeseries_type}_timeseries/asf_rtcs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47523244",
   "metadata": {},
   "source": [
    "Like in the previous notebook, define a variable representing the path to the parent dir of asf_rtcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd74d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = str(pathlib.Path(path_to_rtcs).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac699f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into memory\n",
    "vv_vrt_path = f\"{data_path}/vrt_files/s1_stack_vv.vrt\"\n",
    "vh_vrt_path = f\"{data_path}/vrt_files/s1_stack_vh.vrt\"\n",
    "ls_vrt_path = f\"{data_path}/vrt_files/s1_stack_ls_map.vrt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43986980",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_data_cube = s1_tools.metadata_processor(\n",
    "    vv_path=vv_vrt_path,\n",
    "    vh_path=vh_vrt_path,\n",
    "    ls_path=ls_vrt_path,\n",
    "    rtc_path=path_to_rtcs,\n",
    "    timeseries_type=timeseries_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d54aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_data_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c975234",
   "metadata": {},
   "source": [
    "### 1) Clip to spatial area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9c132",
   "metadata": {},
   "source": [
    "Until now, we've kept the full spatial extent of the dataset. This hasn't been a problem because all of our operations have been lazy. Now, we'd like to visualize the dataset in ways that require eager instead of lazy computation. We subset the data cube to a smaller area to focus on a location of interest and to reduce the computational cost.\n",
    "\n",
    "Later notebooks use a different Sentinel-1 RTC dataset that is accessed for a smaller area of interest. Clip the current data cube to that spatial footprint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ae5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read vector data\n",
    "pc_aoi = gpd.read_file(\"../data/vector_data/hma_rtc_aoi.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df69a0a",
   "metadata": {},
   "source": [
    "Visualize the location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b48c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ad582",
   "metadata": {},
   "source": [
    "Check the CRS and ensure it matches that of the raster data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e708f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert asf_data_cube.rio.crs == pc_aoi.crs, f\"Expected: {asf_data_cube.rio.crs}, received: {pc_aoi.crs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883fddf",
   "metadata": {},
   "source": [
    "Clip the raster data cube by the extent of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube = asf_data_cube.rio.clip(pc_aoi.geometry, pc_aoi.crs)\n",
    "clipped_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765626a",
   "metadata": {},
   "source": [
    "Use [`xr.Dataset.persist()`](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.persist.html); this method is an integration of Dask with Xarray. It will trigger the background computation of the operations we've so far executed lazily. Persist is similar to compute but it keeps the underlying data as dask-backed arrays instead of converting them to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube = clipped_cube.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b28ee7",
   "metadata": {},
   "source": [
    "Great, we've gone from an object where each 3-d variable is ~ 90 GB to one where each 3-d variable is ~45 MB, this will be much easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e8cb7",
   "metadata": {},
   "source": [
    "## B. Layover-shadow map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f06d845",
   "metadata": {},
   "source": [
    "As discussed in previous notebooks, every Sentinel-1 scene comes with an associated layover shadow mask GeoTIFF file. This map describes the presence of layover, shadow and slope angle conditions that can impact backscatter values in a scene, which is especially important to consider in high-relief settings with more potential for geometric distortions\n",
    "\n",
    "The following information is copied from the README file that accompanies each scene: \n",
    "\n",
    "```\n",
    "Layover-shadow mask\n",
    "\n",
    "The layover/shadow mask indicates which pixels in the RTC image have been affected by layover and shadow. This layer is tagged with _ls_map.tif\n",
    "\n",
    "The pixel values are generated by adding the following values together to indicate which layover and shadow effects are impacting each pixel:\n",
    "0.  Pixel not tested for layover or shadow\n",
    "1.  Pixel tested for layover or shadow\n",
    "2.  Pixel has a look angle less than the slope angle\n",
    "4.  Pixel is in an area affected by layover\n",
    "8.  Pixel has a look angle less than the opposite of the slope angle\n",
    "16. Pixel is in an area affected by shadow\n",
    "\n",
    "There are 17 possible different pixel values, indicating the layover, shadow, and slope conditions present added together for any given pixel._\n",
    "\n",
    "The values in each cell can range from 0 to 31:\n",
    "0.  Not tested for layover or shadow\n",
    "1.  Not affected by either layover or shadow\n",
    "3.  Look angle < slope angle\n",
    "5.  Affected by layover\n",
    "7.  Affected by layover; look angle < slope angle\n",
    "9.  Look angle < opposite slope angle\n",
    "11. Look angle < slope and opposite slope angle\n",
    "13. Affected by layover; look angle < opposite slope angle\n",
    "15. Affected by layover; look angle < slope and opposite slope angle\n",
    "17. Affected by shadow\n",
    "19. Affected by shadow; look angle < slope angle\n",
    "21. Affected by layover and shadow\n",
    "23. Affected by layover and shadow; look angle < slope angle\n",
    "25. Affected by shadow; look angle < opposite slope angle\n",
    "27. Affected by shadow; look angle < slope and opposite slope angle\n",
    "29. Affected by shadow and layover; look angle < opposite slope angle\n",
    "31. Affected by shadow and layover; look angle < slope and opposite slope angle\n",
    "\n",
    "```\n",
    "\n",
    "The ASF RTC image [product guide](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/) has detailed descriptions of how the data is processed and what is included in the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78971457",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "clipped_cube.isel(acq_date=1).ls.plot(ax=ax)\n",
    "ax.set_title(\"Layover-shadow map of a single time step\")\n",
    "ax.tick_params(axis=\"x\", labelrotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a228519",
   "metadata": {},
   "source": [
    "The `layover-shadow` variable provides categorical information. Instead of the default 'viridis' colormap, we'll use a qualitative colormap to visualize the data. We'll also plot the layaover-shadow maps of two time steps side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84010653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cmap = plt.get_cmap(\"tab20b\", lut=32)\n",
    "time_step1 = \"2021-06-07\"\n",
    "time_step2 = \"2021-06-10\"\n",
    "\n",
    "if timeseries_type == \"subset\":\n",
    "    time_step1 = \"2021-05-05T00:03:07\"\n",
    "    time_step2 = \"2021-05-14T12:13:49\"\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 7), layout=\"constrained\")\n",
    "\n",
    "clipped_cube.sel(acq_date=time_step1).ls.plot(ax=axs[0], cmap=cat_cmap, cbar_kwargs=({\"label\": None}), vmin=0, vmax=31)\n",
    "\n",
    "clipped_cube.sel(acq_date=time_step2).ls.plot(ax=axs[1], cmap=cat_cmap, cbar_kwargs=({\"label\": None}), vmin=0, vmax=31)\n",
    "\n",
    "fig.suptitle(f\"Layover shadow map of two time steps: {time_step1}, and {time_step2}\", y=1.05)\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_xlabel(None)\n",
    "    axs[i].set_ylabel(None)\n",
    "    axs[i].tick_params(axis=\"x\", labelrotation=45)\n",
    "\n",
    "axs[0].set_title(time_step1)\n",
    "axs[1].set_title(time_step2)\n",
    "fig.supylabel(\"y coordinate of projection (m)\")\n",
    "fig.supxlabel(\"x coordinate of projection (m)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b3cc4",
   "metadata": {},
   "source": [
    "It looks like there are areas affected by different types of distortion on different dates. For example, in the lower left quadrant, there is a region that is blue (5 - affected by layover) on 6/7/2021 but much of that area appear yellow (16 - affected by radar shadow) on 6/10/2021. This pattern is present throughout much of the scene with portions of area that are affected by layover in one acquisition in shadow in the next acquisition. This is not due to any real changes on the ground that occurred between the two acquisitions, rather it is the different viewing geometries of the orbital passes: one of the above scenes was collected during an ascending pass of the satellite and one during a descending pass. Since Sentinel-1 is always looking to the same side, ascending and descending passes will view the same area on the ground from opposing perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f4a7d",
   "metadata": {},
   "source": [
    ":::{attention}\n",
    "If you're following along using the subset time series, your plot will look different than the plot above. That plot should display the layover-shadow map from 05/05/2021 on the left and 05/14/2021 on the right. In this plot, you'll see that the areas in shadow on 05/05/2021 are similar to the areas affected by layover on 05/14/2024.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026214b1",
   "metadata": {},
   "source": [
    "### 1) Interactive visualization of layover-shadow maps\n",
    "We can use Xarray's integration with [hvplot](https://hvplot.holoviz.org/) (a library within the [holoviz](https://hvplot.holoviz.org/index.html) ecosystem) to look at the time series of layover-shadow maps interactively. Read more about interactive plots with Xarray and hvplot [here](https://tutorial.xarray.dev/intermediate/hvplot.html).\n",
    "\n",
    "To do this, we need to demote `'ls'` from a coordinate variable to a data variable because of how `hvplot.xarray` expects the data to be structured. We can do this with [`xr.reset_coords()`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.reset_coords.html#xarray.Dataset.reset_coords). First, recreate the above layover-shadow plot using hvplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_var = clipped_cube.reset_coords(\"ls\")\n",
    "\n",
    "(\n",
    "    ls_var.ls.sel(acq_date=time_step1)\n",
    "    .squeeze()\n",
    "    .hvplot(\n",
    "        cmap=\"tab20b\",\n",
    "        width=400,\n",
    "        height=350,\n",
    "        clim=(0, 32),  # specify the limits of the colorbar to match original\n",
    "        title=f\"Acq date: {time_step1}\",\n",
    "    )\n",
    "    + ls_var.ls.sel(acq_date=time_step2)\n",
    "    .squeeze()\n",
    "    .hvplot(\n",
    "        cmap=\"tab20b\",\n",
    "        width=400,\n",
    "        height=350,\n",
    "        clim=(0, 32),\n",
    "        title=f\"Acq date: {time_step2}\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1210004",
   "metadata": {},
   "source": [
    "This is an interactive version of the side-by-side layover-shadow maps shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de6f05",
   "metadata": {},
   "source": [
    "(content:orbital_dir_section)=\n",
    "## C. Orbital direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e8bcc",
   "metadata": {},
   "source": [
    "Sentinel-1 is a right-looking sensor and it images areas on Earth’s surface in orbits when it is moving N-S (a descending orbit) and S-N (an ascending orbit). For a given scene, it images the same footprint on both passes but from different directions. The data coverage map below illustrates these directional passes with ascending passes moving from southeast to northwest and descending passes moving from northeast to southwest , it can be found online [here](https://asf.alaska.edu/daac/sentinel-1-acquisition-maps/).\n",
    "\n",
    "```{image} ../imgs/slc_coverage_asf.png\n",
    "---\n",
    ":align center\n",
    "---\n",
    "ASF Sentinel-1 Cumulative coverage map. Source: Sentinel-1 Acquisition Maps, [ASF DAAC](https://asf.alaska.edu/daac/sentinel-1-acquisition-maps/).\n",
    "\n",
    "```\n",
    "\n",
    "In areas of high-relief topography such as the area we’re observing, there can be strong terrain distortion effects such as layover and shadow. These are some of the distortions that RTC processing corrects, but sometimes it is not possible to reliably extract backscatter for correction in the presence of strong distortions. The above image shows the layover-shadow map for an ascending and a descending image side-by-side, which is why different areas are affected by layover (5) and shadow (17) in each.\n",
    "\n",
    "Thanks to all the setup work we did in the previous notebook, we can quickly confirm that all of the observations were taken at two times of day, corresponding to ascending and descending passes of the satellite, and that the time steps shown above were taken at different times of day.\n",
    "\n",
    ":::{note}\n",
    "The acquisition time of Sentinel-1 images is not in local time.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Hour of day of acquisition {time_step1}: \",\n",
    "    clipped_cube.sel(acq_date=time_step1).acq_date.dt.hour.data,\n",
    ")\n",
    "print(\n",
    "    f\"Hour of day of acquisition {time_step2}: \",\n",
    "    clipped_cube.sel(acq_date=time_step2).acq_date.dt.hour.data,\n",
    ")\n",
    "clipped_cube.acq_date.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b6dc3",
   "metadata": {},
   "source": [
    "### 1) Is a pass ascending or descending?\n",
    "\n",
    "In this example, it was relatively simple to determine one pass from another, but it is less straightforward to know if a pass is ascending or descending. The timing of these passes depends on the location on earth of the image. \n",
    "\n",
    "In the location covered by this dataset, ascending passes correspond to an acquisition time roughly 0:00 UTC and descending passes correspond to approximately 12:00 UTC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409f2f2",
   "metadata": {},
   "source": [
    "### 2) Assign orbital direction as a coordinate variable\n",
    "This is another example of time-varying metadata, so it should be stored as a coordinate variable. Use [`xr.where()`](https://docs.xarray.dev/en/stable/generated/xarray.where.html) to assign the correct orbital direction value depending on an observation's acquisition time and then assign it as a coordinate variable to the clipped raster data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.coords[\"orbital_dir\"] = (\n",
    "    \"acq_date\",\n",
    "    xr.where(clipped_cube.acq_date.dt.hour.data == 0, \"asc\", \"desc\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba95d2",
   "metadata": {},
   "source": [
    "## D. Duplicate time steps\n",
    "\n",
    ":::{note}\n",
    "If you're working with the subset time series, this section **will not work** because it is about addressing irregularities present in observations of the full time series. Skip ahead to [Section E](#e-examine-coverage-over-time-series).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407bada",
   "metadata": {},
   "source": [
    "If we take a closer look at the ASF dataset, we can see that there are a few scenes from identical acquisitions (this is apparent in `acq_date` and more specifically in `product_id`). Let's examine these and see what's going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2580b08",
   "metadata": {},
   "source": [
    "### 1) Identify duplicate time steps\n",
    "\n",
    "First we'll extract the `data_take_ID` from the Sentinel-1 granule ID: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cfec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.data_take_ID.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895feee",
   "metadata": {},
   "source": [
    "Let's look at the number of unique elements using [`np.unique()`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_take_ids_ls = clipped_cube.data_take_ID.data.tolist()\n",
    "data_take_id_set = np.unique(clipped_cube.data_take_ID)\n",
    "len(data_take_id_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b6456",
   "metadata": {},
   "source": [
    "Interesting - it looks like there are only 96 unique elements. Let's figure out which are duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate(input_ls):\n",
    "    return list(set([x for x in input_ls if input_ls.count(x) > 1]))\n",
    "\n",
    "\n",
    "duplicate_ls = duplicate(data_take_ids_ls)\n",
    "duplicate_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a310429",
   "metadata": {},
   "source": [
    "These are the data take IDs that are duplicated in the dataset. We now want to subset the xarray object to only include these data take IDs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_duplicate_cond = clipped_cube.data_take_ID.isin(duplicate_ls)\n",
    "asf_duplicate_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0841d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_cube = clipped_cube.where(asf_duplicate_cond == True, drop=True)\n",
    "duplicates_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ffdd8",
   "metadata": {},
   "source": [
    "### 2) Visualize duplicates\n",
    "\n",
    "Great, now we have a 12-time step Xarray object that contains only the duplicate data takes. Let's see what it looks like. We can use `xr.FacetGrid` objects to plot all of the arrays at once.\n",
    "\n",
    "Before we make a FacetGrid plot, we need to make a change to the dataset. FacetGrid takes a column and expands the levels of the provided dimension into individual sub-plots (a small multiples plot). We're looking at the duplicate time steps, meaning the elements of the `acq_date` dimension are non-unique. FacetGrid expects unique values along the specified coordinate array. If we were to directly call: \n",
    "```python\n",
    "fg = duplicates_cube.vv.plot(col=\"acq_date\", col_wrap=4)\n",
    "``` \n",
    "We would receive the following error: \n",
    "```\n",
    "ValueError: Coordinates used for faceting cannot contain repeated (nonunique) values.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d4ff9",
   "metadata": {},
   "source": [
    "Renaming the dimensions of `duplicates_cube` with [`xr.rename_dims()`](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.rename_dims.html) demotes the `acq_date` coordinate array to non-dimensional coordinate and replaces it with `step` an array of integers. Because these are unique, we can make a FaceGrid plot with the `step` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_cube.rename_dims({\"acq_date\": \"step\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63393b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = duplicates_cube.rename_dims({\"acq_date\": \"step\"}).vv.plot(col=\"step\", col_wrap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f8518",
   "metadata": {},
   "source": [
    "Interesting, it looks like there's only really data for the 0, 2, 4, 7 and 9 elements of the list of duplicates. It could be that the processing of these files was interrupted and then restarted, producing extra empty arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412553bf",
   "metadata": {},
   "source": [
    "### 3) Drop duplicates\n",
    "\n",
    "To drop these arrays, extract the product ID (the only variable that is unique among the duplicates) of each array we'd like to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58678828",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ls = [1, 3, 5, 6, 8, 10, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5e524",
   "metadata": {},
   "source": [
    "We can use xarray's `.isel()` method, `.xr.DataArray.isin()`, `xr.Dataset.where()`, and list comprehension to efficiently subset the time steps we want to keep: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_product_id_ls = duplicates_cube.isel(acq_date=drop_ls).product_id.data\n",
    "drop_product_id_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9bab7",
   "metadata": {},
   "source": [
    "Using this list, we want to drop all of the elements of `clipped_cube` where product ID is one of the values in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_cond = ~clipped_cube.product_id.isin(drop_product_id_ls)\n",
    "clipped_cube = clipped_cube.where(duplicate_cond == True, drop=True)\n",
    "clipped_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4fde7",
   "metadata": {},
   "source": [
    "## E. Examine coverage over time series\n",
    "\n",
    "The previous section showed that there are some scenes in the time series with no data over the area of interest. Let's see if there are any data coverage characteristics we should examine more closely in the dataset. There are a number of ways to do this but approach we'll use here is to first calculate the percentage of pixels containing data relative to the entire footprint for each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4497b",
   "metadata": {},
   "source": [
    "### 1) Calculate coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pixels = clipped_cube.ls.notnull().any(\"acq_date\").sum([\"x\", \"y\"])\n",
    "valid_pixels = clipped_cube.ls.count(dim=[\"x\", \"y\"])\n",
    "clipped_cube[\"cov\"] = (valid_pixels / max_pixels) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0436d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Distribution of percentage of valid pixels for \\n each time step of backscatter time series\")\n",
    "clipped_cube[\"cov\"].plot.hist(ax=ax)\n",
    "ax.set_xlabel(\"% Coverage\")\n",
    "ax.set_title(None)\n",
    "ax.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee87884",
   "metadata": {},
   "source": [
    "In addition to the empty duplicate time steps we removed above, there are many other time steps with minimal coverage. This is likely because the original time series contains scenes from multiple satellite footprints, and some may have minimal coverage over the area of interest that was used to clip the time series. To check this, we can again use Xarray's interactive visualization tools, this time to create an animation that loops through the time series. We'll get a bunch of warnings and errors if we try to make an animation of a time series with empty time steps, so we'll first remove any empty time steps (likely also caused by satellite footprints in the original time series that don't cover the smaller area of interest) and look just at scenes with some data:\n",
    "\n",
    ":::{admonition} Masking with dask-backed arrays\n",
    "If we tried to use `xr.Dataset.where()` to mask and drop time steps where coverage is zero right now, we would get the following error: \n",
    "```\n",
    "KeyError: 'Indexing with a boolean dask array is not allowed. This will result in a dask array of unknown shape. Such arrays are unsupported by Xarray.Please compute the indexer first using .compute()'\n",
    "```\n",
    "Because we only called `.persist()` earlier, the arrays of the data variables are still Dask arrays instead of NumPy arrays. Calling `compute()` or `.load()` will resolve this issue and allow us to drop empty time steps: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd40960",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube = clipped_cube.where(clipped_cube.cov > 0.0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3f946",
   "metadata": {},
   "source": [
    "Now that we've dropped the empty time steps, we can look at the layover shadow map at a few time steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layover-shadow map for first 10 scenes in the time series (full time series) and 3 scenes (subset time series)\n",
    "if timeseries_type == \"full\":\n",
    "    clipped_cube.ls.isel(acq_date=slice(0, 10)).plot(col=\"acq_date\", col_wrap=5, cmap=\"tab20b\", vmin=0, vmax=32)\n",
    "elif timeseries_type == \"subset\":\n",
    "    clipped_cube.ls.plot(col=\"acq_date\", cmap=\"tab20b\", vmin=0, vmax=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72babc57",
   "metadata": {},
   "source": [
    "In the small multiples plot above, it looks like there are two main viewing geometries in the time series, and that one  of them covers the area of interest well while the other does not. Let's remove the time steps where coverage is less than 100. We won't need the `'cov'` variable anymore so we can drop it as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bd8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube = clipped_cube.where(clipped_cube.cov == 100, drop=True).drop_vars(\"cov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfea7cd",
   "metadata": {},
   "source": [
    "If we remake the plots of the layover-shadow maps for the first ten time steps, we'll see that they have the same viewing geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7037dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layover-shadow map for first 10 scenes with full coverage (full time series)\n",
    "# if using subset time series, plot layover-shadow map for the 2 scenes with full coverage\n",
    "if timeseries_type == \"full\":\n",
    "    clipped_cube.ls.isel(acq_date=slice(0, 10)).plot(col=\"acq_date\", col_wrap=5, cmap=\"tab20b\", vmin=0, vmax=32)\n",
    "elif timeseries_type == \"subset\":\n",
    "    clipped_cube.ls.plot(col=\"acq_date\", cmap=\"tab20b\", vmin=0, vmax=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d265b2",
   "metadata": {},
   "source": [
    "The layover-shadow maps for the first ten time steps all show the same viewing geometry. You can see two main 'modes' of areas that are affected by layover (value = 5) and shadow (value = 16). These correspond to acquisitions from ascending and descending passes of the satellite. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14382448",
   "metadata": {},
   "source": [
    "## F. Data visualization\n",
    "\n",
    "Now that we've visualized and taken a closer look at metadata such as the layover-shadow map and orbital direction, let's focus on backscatter variability. In the plotting calls in this notebook, we'll use a function in `s1_tools.py` that applies a logarithmic transformation to the data. This makes it easier to visualize variability, however, it is important to apply it as a last step before visualizing and to not perform statistical calculations on the log-transformed data.\n",
    "\n",
    ":::{admonition} A note on visualizing SAR data\n",
    "The measurements that we're provided in the RTC dataset are in intensity, or power, scale. Often, to visualize SAR backscatter, the data is converted from power to normalized radar cross section (the backscatter coefficient). This is in decibel (dB) units, meaning a log transform has been applied. This transformation makes it easier to visualize variability but it is important not to calculate summary statistics on log-transformed data as it will be distorted. You can read more about these concepts [here](https://hyp3-docs.asf.alaska.edu/guides/introduction_to_sar/#sar-scale).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9fdd6",
   "metadata": {},
   "source": [
    "### 1) Mean backscatter over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c66fe",
   "metadata": {},
   "source": [
    "This section examines approaches of plotting VV and VH backscatter side by side that can have important consequences when visualizing and interpreting data. \n",
    "\n",
    "Currently, VV and VH are two data variables of the data cube that both exist along x,y and time dimensions. If we plot them both as individual subplots, we'll see two subplots, each with their own colormap. If you look closely, you can see that the colormaps are not on the same scale. We need to be careful when interpreting these images and comparing backscatter in the VV and VH images in this situation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c5040",
   "metadata": {},
   "source": [
    "In addition, we know from the layover-shadow maps that different areas are affected by shadow and masked from the dataset in both ascending and descending passes. For a careful examination of backscatter, we'll look at the mean over time of ascending scenes and mean over time of descending scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset time series by ascending and descending passes\n",
    "asc_pass_cond = clipped_cube.orbital_dir == \"asc\"\n",
    "asc_pass_obs = clipped_cube.sel(acq_date=asc_pass_cond)\n",
    "\n",
    "desc_pass_cond = clipped_cube.orbital_dir == \"desc\"\n",
    "desc_pass_obs = clipped_cube.sel(acq_date=desc_pass_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12, 12), layout=\"constrained\")\n",
    "\n",
    "s1_tools.power_to_db(asc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[0][0], cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"})\n",
    ")\n",
    "s1_tools.power_to_db(asc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[0][1], cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"})\n",
    ")\n",
    "\n",
    "s1_tools.power_to_db(desc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[1][0], cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"})\n",
    ")\n",
    "s1_tools.power_to_db(desc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[1][1], cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"})\n",
    ")\n",
    "\n",
    "for i in range(len(ax[0])):\n",
    "    for j in range(len(ax)):\n",
    "        ax[j][i].set_ylabel(None)\n",
    "        ax[j][i].set_xlabel(None)\n",
    "        ax[j][i].tick_params(axis=\"x\", labelrotation=45)\n",
    "fig.suptitle(\n",
    "    \"Mean backscatter over time broken up by polarization and orbital direction\",\n",
    "    fontsize=16,\n",
    ")\n",
    "ax[0][0].set_title(\"VV - Ascending passes\")\n",
    "ax[0][1].set_title(\"VH - Ascending passes\")\n",
    "ax[1][0].set_title(\"VV - Descending passes\")\n",
    "ax[1][1].set_title(\"VH - Descending passes\")\n",
    "fig.supylabel(\"y coordinate of projection (m)\")\n",
    "fig.supxlabel(\"x coordinate of projection (m)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba608d",
   "metadata": {},
   "source": [
    "This figure shows mean backscatter for both polarizations and separated into ascending and descending passes. However, as stated above, each sub-plot has its own colormap, making it difficult to compare across the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d4dd1",
   "metadata": {},
   "source": [
    "There are two ways to fix the problem of mismatched color map scales: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebcda0",
   "metadata": {},
   "source": [
    "#### Specify min and max values in plotting call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0138e9",
   "metadata": {},
   "source": [
    "We could normalize the backscatter ranges for both variables by manually specifying a minimum and a maximum across both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58294317",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min = np.array(\n",
    "    [\n",
    "        s1_tools.power_to_db(asc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).min(),\n",
    "        s1_tools.power_to_db(asc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).min(),\n",
    "        s1_tools.power_to_db(desc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).min(),\n",
    "        s1_tools.power_to_db(desc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).min(),\n",
    "    ]\n",
    ").min()\n",
    "\n",
    "global_max = np.array(\n",
    "    [\n",
    "        s1_tools.power_to_db(asc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).max(),\n",
    "        s1_tools.power_to_db(asc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).max(),\n",
    "        s1_tools.power_to_db(desc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).max(),\n",
    "        s1_tools.power_to_db(desc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).max(),\n",
    "    ]\n",
    ").max()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12, 12), layout=\"constrained\")\n",
    "\n",
    "s1_tools.power_to_db(asc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[0][0],\n",
    "    cmap=plt.cm.Greys_r,\n",
    "    vmin=global_min,\n",
    "    vmax=global_max,\n",
    "    cbar_kwargs=({\"label\": \"dB\"}),\n",
    ")\n",
    "s1_tools.power_to_db(asc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[0][1],\n",
    "    cmap=plt.cm.Greys_r,\n",
    "    vmin=global_min,\n",
    "    vmax=global_max,\n",
    "    cbar_kwargs=({\"label\": \"dB\"}),\n",
    ")\n",
    "s1_tools.power_to_db(desc_pass_obs[\"vv\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[1][0],\n",
    "    cmap=plt.cm.Greys_r,\n",
    "    vmin=global_min,\n",
    "    vmax=global_max,\n",
    "    cbar_kwargs=({\"label\": \"dB\"}),\n",
    ")\n",
    "\n",
    "s1_tools.power_to_db(desc_pass_obs[\"vh\"].mean(dim=\"acq_date\")).plot(\n",
    "    ax=ax[1][1],\n",
    "    cmap=plt.cm.Greys_r,\n",
    "    vmin=global_min,\n",
    "    vmax=global_max,\n",
    "    cbar_kwargs=({\"label\": \"dB\"}),\n",
    ")\n",
    "\n",
    "for i in range(len(ax[0])):\n",
    "    for j in range(len(ax)):\n",
    "        ax[j][i].set_ylabel(None)\n",
    "        ax[j][i].set_xlabel(None)\n",
    "        ax[j][i].tick_params(axis=\"x\", labelrotation=45)\n",
    "fig.suptitle(\n",
    "    \"Mean backscatter over time broken up by polarization and orbital direction\",\n",
    "    fontsize=16,\n",
    ")\n",
    "ax[0][0].set_title(\"VV - Ascending passes\")\n",
    "ax[0][1].set_title(\"VH - Ascending passes\")\n",
    "ax[1][0].set_title(\"VV - Descending passes\")\n",
    "ax[1][1].set_title(\"VH - Descending passes\")\n",
    "fig.supylabel(\"y coordinate of projection (m)\")\n",
    "fig.supxlabel(\"x coordinate of projection (m)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1b66f",
   "metadata": {},
   "source": [
    "Great, we now have the same four plots all generated using a common scale. However, this took a bit of extra work and lines of code. A cleaner option is to expand the dimensions of the data cube, shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab4dc5",
   "metadata": {},
   "source": [
    "#### Expand dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63436945",
   "metadata": {},
   "source": [
    "We could convert the VV and VH data from being represented as data variables to as elements of a band dimension. This would let us use Xarray's FacetGrid plotting that creates small multiples along a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand dimensions to include band\n",
    "clipped_cube_da = clipped_cube.to_array(dim=\"band\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a79f6f",
   "metadata": {},
   "source": [
    "Since `orbital_dir` is a coordinate that exists along the time dimension, we do not want to expand the dimensions of the dataset. Instead, use `groupby()` to separate those observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1279e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = s1_tools.power_to_db(clipped_cube_da.groupby(\"orbital_dir\").mean(dim=\"acq_date\")).plot(\n",
    "    col=\"band\", row=\"orbital_dir\", cmap=plt.cm.Greys_r, cbar_kwargs={\"label\": \"dB\"}\n",
    ")\n",
    "f.axs[1][0].tick_params(axis=\"x\", labelrotation=45)\n",
    "f.axs[1][1].tick_params(axis=\"x\", labelrotation=45)\n",
    "f.fig.supylabel(\"y coordinate of projection (m)\")\n",
    "f.fig.supxlabel(\"x coordinate of projection (m)\")\n",
    "f.fig.suptitle(\n",
    "    \"Mean backscatter over time for VV and VH polarizations and \\n ascending and descending passes with FacetGrid\",\n",
    "    fontsize=16,\n",
    "    y=1.05,\n",
    ")\n",
    "f.fig.set_figheight(12)\n",
    "f.fig.set_figwidth(12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37370f5",
   "metadata": {},
   "source": [
    "The above plot shows the areas impacted by shadow in both ascending and descending returns. The time of day of the acquisition and the orientation of the imaged terrain with respect to the sensor are the only differences between ascending and descending backscatter images. Some studies have even exploited the different timing of ascending and descending passes in order to examine diurnal processes as melt-freeze cycles in an alpine snowpack {cite}`lund_2020_snowmelt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5700024",
   "metadata": {},
   "source": [
    "#### Creating a gap-filled backscatter image\n",
    "\n",
    "In the above plots, we separate the observations by orbital direction in order to compare specific imaging conditions (eg. time of day) and to be able to observe the timing and location of conditions like shadow that impact backscatter. This is a helpful approach when we want a detailed understanding of scattering conditions in a given area. At other times, we may be more interested in building a complete backscatter image. Below, we'll show the same plots as above but combining ascending and descending pass scenes instead of keeping them separate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f43b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = s1_tools.power_to_db(clipped_cube_da.mean(dim=\"acq_date\")).plot(\n",
    "    col=\"band\", cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"})\n",
    ")\n",
    "\n",
    "f.fig.suptitle(\"Mean backscatter over time for VV and VH polarizations with FacetGrid\")\n",
    "f.fig.set_figheight(7)\n",
    "f.fig.set_figwidth(12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1353e2",
   "metadata": {},
   "source": [
    "The area that we're looking at is in the mountainous region on the border between the Sikkim region of India and China. There are four north-facing glaciers visible in the image, each with a lake at the toe. Bodies of water like lakes tend to appear dark in C-band SAR images because water is smooth with respect to the wavelength of the signal, meaning that most of the emitted signal is scattered away from the sensor. When surfaces are rough at the scale of C-band wavelength, more signal is returned to the sensor and the backscatter image is brighter. For much more detail on interpreting SAR imagery, see the resources linked in the Sentinel-1 section of the [tutorial data](../../background/4_tutorial_data.md) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41abc9-9d67-4ef7-8a23-ea2699cb8920",
   "metadata": {},
   "source": [
    "### 2) Seasonal backscatter variability\n",
    "\n",
    "Now let's look at how backscatter may vary seasonally for a single polarization (for more on time-related GroupBy operations see the [Xarray User Guide](https://docs.xarray.dev/en/stable/user-guide/time-series.html#resampling-and-grouped-operations)). This is an example of a 'split-apply-combine' operation, where a dataset is split into groups (in this case, time steps are split into seasons), an operation is applied (in this case, the mean is calculated) and then the groups are combined into a new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932cee5-a9d4-49da-95bc-386dffcad6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube_gb = clipped_cube.groupby(\"acq_date.season\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02768bb",
   "metadata": {},
   "source": [
    "The temporal dimension of the new object has an element for each season rather than an element for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee55f9-130b-46bd-a4ba-6d5a24669885",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b05f5e-aca3-4ec0-938a-3c775724a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order seasons correctly\n",
    "clipped_cube_gb = clipped_cube_gb.reindex({\"season\": [\"DJF\", \"MAM\", \"JJA\", \"SON\"]})\n",
    "clipped_cube_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f020fd",
   "metadata": {},
   "source": [
    "Visualize mean backscatter in each season:\n",
    "\n",
    ":::{note}\n",
    "If you're working with the subset time series, these plots will not display correctly because only one season is represented.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a662aa5-eb13-44fe-8c61-c9b38a96338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_vv = s1_tools.power_to_db(clipped_cube_gb.vv).plot(col=\"season\", cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd42b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_vh = s1_tools.power_to_db(clipped_cube_gb.vh).plot(col=\"season\", cmap=plt.cm.Greys_r, cbar_kwargs=({\"label\": \"dB\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb2508",
   "metadata": {},
   "source": [
    "The glacier surfaces appear much darker during the summer months compared to other seasons, especially in the lower reaches of the glaciers. Like the lake surfaces above, this suggests largely specular reflection where no signal returns in the incident direction. It could be that during the summer months, enough liquid water is present at the glacier surface to produce this scattering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fd823",
   "metadata": {},
   "source": [
    "### 3) Backscatter time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "s1_tools.power_to_db(clipped_cube[\"vv\"].sel(acq_date=asc_pass_cond).mean(dim=[\"x\", \"y\"])).plot.scatter(\n",
    "    ax=ax, label=\"VV - asc\", c=\"b\", marker=\"o\"\n",
    ")\n",
    "s1_tools.power_to_db(clipped_cube[\"vv\"].sel(acq_date=desc_pass_cond).mean(dim=[\"x\", \"y\"])).plot.scatter(\n",
    "    ax=ax, label=\"VV - desc\", c=\"b\", marker=\"x\"\n",
    ")\n",
    "s1_tools.power_to_db(clipped_cube[\"vh\"].sel(acq_date=asc_pass_cond).mean(dim=[\"x\", \"y\"])).plot.scatter(\n",
    "    ax=ax, label=\"VH - asc\", c=\"r\", marker=\"o\"\n",
    ")\n",
    "s1_tools.power_to_db(clipped_cube[\"vh\"].sel(acq_date=desc_pass_cond).mean(dim=[\"x\", \"y\"])).plot.scatter(\n",
    "    ax=ax, label=\"VH - desc\", c=\"r\", marker=\"x\"\n",
    ")\n",
    "fig.legend(loc=\"center right\")\n",
    "\n",
    "fig.suptitle(\"Backscatter variability over time by polarization and orbital direction\")\n",
    "ax.set_title(None)\n",
    "ax.set_ylabel(\"dB\")\n",
    "ax.set_xlabel(\"Time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881ad14",
   "metadata": {},
   "source": [
    "To make an interactive plot of backscatter variability, use [`hvplot`](https://tutorial.xarray.dev/intermediate/hvplot.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a69f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_plot = s1_tools.power_to_db(clipped_cube[\"vv\"].mean(dim=[\"x\", \"y\"])).hvplot.scatter(x=\"acq_date\")\n",
    "vh_plot = s1_tools.power_to_db(clipped_cube[\"vh\"].mean(dim=[\"x\", \"y\"])).hvplot.scatter(x=\"acq_date\")\n",
    "\n",
    "vv_plot * vh_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90cc3cb",
   "metadata": {},
   "source": [
    "The time series plots show seasonal variability in backscatter for both VV and VH polarizations. You can see backscatter minima in both polarizations during the summer, which is consistent with the seasonal variability shown in the small multiples plots above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130494d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated using the data cube we assembled in the previous notebooks. We saw various ways that our learning about the dataset was smoother and more efficient because of the data cube's organization with accessible metadata attached to appropriate dimensions.\n",
    "\n",
    "\n",
    "In the next notebook, we'll work with a different Sentinel-1 RTC dataset. The final notebook of this tutorial includes a comparison between the ASF Sentinel-1 backscatter cube and the Planetary Computer Sentinel-1 backscatter cube that we'll see in [notebook 4](4_read_pc_data.ipynb). \n",
    "\n",
    ":::{note}\n",
    "Rather than repeat the work we just did in this notebook in the dataset comparison notebook, we will work with a copy of `clipped_cube` that we read from disk. **This is a Zarr data cube that is provided in the GitHub repo.** If you worked with the full dataset in this notebook, you can go through the steps of writing the data to disk by executing the cell below. \n",
    "\n",
    "If you used the subset time series in this notebook, you do not want to run the cell because it will rewrite the clipped version of the full time series downloaded when you cloned the repo, which is needed for the comparison with the Planetary Computer Dataset. If this happens, don't worry, you can download `s1_asf_clipped_cube.zarr` again from the GitHub repo. \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22671cc4-5e5e-40c1-90d2-2a74290b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with a different path if you want to write the data to a different location\n",
    "s1_cube_write_location = f\"../data/raster_data/full_timeseries/intermediate_cubes/s1_asf_clipped_cube.zarr\"\n",
    "\n",
    "# Only write to disk if clipped_cube is the full time series\n",
    "if timeseries_type == \"full\":\n",
    "    clipped_cube.to_zarr(\n",
    "        s1_cube_write_location,\n",
    "        mode=\"w\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Write operation not performed. clipped_cube is a subset of the time series, not the full timeseries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
