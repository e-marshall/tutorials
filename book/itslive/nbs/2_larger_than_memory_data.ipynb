{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.2 Working with larger than memory data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll read an ITS_LIVE data cube like in the previous notebook, this time focusing on strategies and approaches for working with such a large dataset *in memory*.\n",
    "\n",
    ":::{tip} \n",
    "If you're not familiar with terms like Dask, chunking and parallelization, we *highly* suggest checking out the brief overview in [Relevant Concepts](../../background/6_relevant_concepts.md) and the resources linked therein.\n",
    ":::\n",
    "{{break}}\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content.Section_A)=\n",
    "**[A. Compare approaches for reading larger than memory data](#a-compare-approaches-for-reading-larger-than-memory-data)**\n",
    "- 1) Calling `xr.open_dataset()` with `chunks = 'auto'`\n",
    "- 2) Calling `xr.open_dataset()` with `chunks = {}`\n",
    "- 3) An out-of-order time dimension\n",
    "- 4) Read the dataset without Dask\n",
    "\n",
    "(content.Section_B)=\n",
    "**[B. Organize data once its in memory](#b-organize-data-once-its-in-memory)**\n",
    "- 1) Arrange dataset in chronological order\n",
    "- 2) Convert to a Dask-backed `Xarray.Dataset`\n",
    "\n",
    "::: \n",
    "\n",
    ":::{tab-item} Learning Goals\n",
    "\n",
    "#### Concepts\n",
    "- Characteristics of larger than memory gridded data\n",
    "- 'Lazy' v. 'non-lazy' operations\n",
    "\n",
    "#### Techniques\n",
    "- Read + write large data with [Xarray](https://xarray.dev/), [Zarr](https://zarr.readthedocs.io/en/stable/), and [Dask](https://www.dask.org/)\n",
    "- Label-based indexing and selection\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import inspect\n",
    "import warnings\n",
    "\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Compare approaches for reading larger than memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses functions we defined in the data access notebook, all of which are stored in the `itslive_tools.py` file. If you cloned this tutorial from its github [repository](https://github.com/e-marshall/cloud-open-source-geospatial-datacube-workflows) you'll see that `itslive_tools.py` is in the same directory as our current notebook, so we can import it with the following line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itslive_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the catalog again, and use the `find_granule_by_point()` to find the URL that points to the ITS_LIVE granule covering your area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive_catalog = gpd.read_file(\"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\")\n",
    "# Find urls for granule covering point of interest\n",
    "url = itslive_tools.find_granule_by_point([95.180191, 30.645973])\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that we defined in the previous notebook, `read_in_s3()`, supports different options for reading large, chunked raster datasets. Before we use that again in this notebook, we will explore these options and the ways that they can impact how we work with the data. You can learn more about reading Zarr data with Xarray [here](https://docs.xarray.dev/en/stable/user-guide/io.html#zarr), and see the different chunking options that are supported and which we will demonstrate below [here](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) `chunks = 'auto'`\n",
    "\n",
    "This is the default option in `read_in_s3()`. The Xarray documentation states that `chunks='auto'` uses \"dask `auto` chunking, taking into account the engine preferred chunks\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto = xr.open_dataset(url, engine=\"zarr\", chunks=\"auto\")\n",
    "dc_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance, the chunks of the object created with `xr.open_dataset(..., chunks='auto')` are a multiple of the on-disk chunk sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [data structure](1_accessing_itslive_s3_data.ipynb#a-overview-of-its_live-data) section of the previous notebook discussed scalar information that is stored as attributes attached to Xarray objects. Similarly, Xarray objects read from Zarr data cubes have associated `encodings` that tell Xarray how to read and write the object to disk. We can use the encoding to learn about preferred chunking schemes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto[\"v\"].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto[\"v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the encoding of the `v` variable, it looks like the chunking scheme is expected to be `{'mid_date': 2000, 'y':10, 'x':10}`. However, the chunks for this variable created with `chunks='auto'` are `{'mid_date': 47892, 'y': 20, 'x': 20}`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the encoding for a 1-dimensional variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto[\"vx_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto[\"vx_error\"].encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. We see that:\n",
    "- The chunk size specified in the encoding doesn't match the total length of the `mid_date` dimension. It may be an artifact from an earlier step in the data processing chain before some observations were eliminated.\n",
    "- The encoding specifies a single chunk along the `mid_date` dimension for this variable, which matches the object we read into memory, the size of this chunk is just different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is that it looks like some of the variables within this `xr.Dataset` have different chunk sizes on the `y` dimension (Shown by  the error produced below). We will need to address this later before rechunking the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto.chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto = dc_auto.unify_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_auto.chunksizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) `chunks = {}`\n",
    "\n",
    "For this argument, the documentation says: \"loads the data with dask using the engine’s preferred chunk size, generally identical to the format’s chunk size. If not available, a single chunk for all arrays.\"\n",
    "\n",
    "Note that with this dataset, `'auto'` and `{}` don't return the same chunking scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set = xr.open_dataset(url, engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set[\"v\"].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set[\"v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, we see that the chunking on the 3-dimensional variable we looked at above ('v') *does* match the chunking specified in the object's encoding: `{'mid_date': 20000, 'y': 10, 'x': 10}`. \n",
    "\n",
    "Looking at a one-dimensional variable, we see the same occurrence as with `dc_auto`: the number of chunks matches what is specified in the encoding, but the size of the chunk is different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set[\"vx_error\"].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set[\"vx_error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `v` and `vx_error` variables shown above have different chunk sizes along the `mid_date` dimension, so we can expect the same chunk sizes error as above, but this time for `mid_date`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set.chunksizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this time, if try to resolve the above error like we did for `dc_auto` (by calling `dc_set = dc_set.unify_chunks()`), We would get a performance warning about the number of chunks increasing by a factor of 186, which isn't great either.\n",
    "\n",
    "```\n",
    " PerformanceWarning: Increasing number of chunks by factor of 186_, chunked_data = chunkmanager.unify_chunks(*unify_chunks_args)\n",
    "```\n",
    "\n",
    " In the next sections, we'll see another option for reading the data into memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) An out-of-order time dimension\n",
    "\n",
    "When we read this dataset from the S3 bucket, we get an object where the time dimension is not in chronological order. Because the dataset is so large, fixing this is not entirely straightforward.\n",
    "\n",
    ":::{tip}\n",
    "It's always a good idea to look at the data!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_set.mid_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard approach would be calling Xarray's [`.sortby()`]((https://docs.xarray.dev/en/stable/generated/xarray.Dataset.sortby.html)) method:\n",
    "```python\n",
    "dc_set = dc_set.sortby('mid_date')\n",
    "```\n",
    "\n",
    "Performing an operation like sorting or slicing requires the entire array to be loaded into memory; for a large dimension like `mid_date` (~48,000 elements), would be very slow and/or would max out available computational resources. \n",
    "\n",
    "There may be a chunking strategy that successfully allows one to sort this dataset along the `mid_date` dimension, but when I tried a few different re-chunking approaches, they did not work. Instead, the successful approach I found was a bit counterintuitive: Re-read the dataset into memory *without* dask. This let's us use Xarray's 'lazy indexing' functionality; we can sort the dataset without loading it into memory. The object will still be quite large so we will chunk the data, incorporating dask, after we sort by the time dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Read the dataset without Dask\n",
    "\n",
    "We'll again use the `read_in_s3()` function, but this time passing `chunks_arg = None`. This is the same as running: `dc = xr.open_dataset(url, engine='Zarr')`. The `read_in_s3()` signature is shown below as a reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = inspect.signature(itslive_tools.read_in_s3)\n",
    "print(signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = itslive_tools.read_in_s3(url, chunks=None)\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, the `mid_date` dimension is still out of order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.mid_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Organize data once it's in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Arrange dataset in chronological order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now, we can lazily perform the `.sortby()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.sortby(\"mid_date\")\n",
    "dc.mid_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! After some experimentation with different approaches, we have our dataset sorted in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Convert to a Dask-backed `Xarray.Dataset`\n",
    "\n",
    "Not passing a `'chunks'` argument to `xr.open_dataset()` means that the Xarray object is a collection of Numpy arrays rather than Dask arrays. However, the dataset is still very large: there are 60 variables that exist along 1,2 or, 3 dimensions (with the exception of the `mapping` variable which we will discuss later), and a single 3-d variable is 123 GB. We will need to use Dask even though we didn't read it in as a collection of Dask arrays. We'll use the preferred chunking from `.encoding['chunks']` to specify a chunking scheme to the object and convert the underlying arrays from Numpy to Dask. \n",
    "\n",
    "Take a look at the `'chunks'`/`'preferred_chunks'` encoding for a 3-d variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc[\"v\"].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_dict = {\"x\": 10, \"y\": 10, \"mid_date\": 20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_rechunk = dc.chunk(chunking_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_rechunk.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we identified a strategy for reading, chunking, and organizing this dataset that works within the memory constraints of my laptop and the size of the data. In the next notebook, we use vector data to narrow our focus in on a spatial area of interest and start examining ice velocity data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
