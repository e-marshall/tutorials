{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a585e7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{title_its_nb1}}\n",
    "\n",
    "{{intro}}\n",
    "This notebook demonstrates how to query and access cloud-hosted Inter-mission Time Series of Land Ice Velocity and Elevation ([ITS_LIVE](https://its-live.jpl.nasa.gov/#access)) data from Amazon Web Services (AWS) S3 buckets. These data are stored as [Zarr](https://zarr.readthedocs.io/en/stable/) data cubes, a cloud-optimized format for array data. They are read into memory as [Xarray](https://docs.xarray.dev/en/stable/) Datasets.\n",
    "\n",
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af38af9",
   "metadata": {
    "tags": []
   },
   "source": [
    "::::{tab-set}   \n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content:section_A)= \n",
    "**[A. Overview of ITS_LIVE data](#a-overview-of-its_live-data)**\n",
    "- {{a1_its_nb1}}\n",
    "- {{a2_its_nb1}}\n",
    "\n",
    "(content:Section_B)=\n",
    "**[B. Read ITS_LIVE data from AWS S3 using Xarray](#b-read-its_live-data-from-aws-s3-using-xarray)**\n",
    "- {{b1_its_nb1}}\n",
    "- {{b2_its_nb1}}\n",
    "- {{b3_its_nb1}}\n",
    "\n",
    "(content:Section_C)=\n",
    "**[C. Query ITS_LIVE catalog](#c-query-its_live-catalog)**\n",
    "\n",
    "- {{c1_its_nb1}}\n",
    "- {{c2_its_nb1}}\n",
    ":::\n",
    "\n",
    ":::{tab-item} Learning Goals\n",
    "{{concepts}}\n",
    "- Understand how data is organized in AWS S3 buckets,\n",
    "- Query and access cloud-optimized dataset from cloud object storage,\n",
    "- Create a vector data object representing the footprint of a raster dataset,\n",
    "- Preliminary visualization of data extent,\n",
    "  \n",
    "{{techniques}}\n",
    "- Use [Xarray](https://xarray.dev/) to open [Zarr](https://zarr.readthedocs.io/en/stable/) datacube stored in [AWS S3 bucket](https://aws.amazon.com/s3/),\n",
    "- Interactive data visualization with [hvplot](https://hvplot.holoviz.org/),\n",
    "- Create [Geopandas](https://geopandas.org/en/stable/) `geodataframe` from Xarray `xr.Dataset` object,\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad5a15",
   "metadata": {
    "tags": []
   },
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc380c8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169eebf3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import xarray as xr\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73cf7d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A. Overview of ITS_LIVE data\n",
    "\n",
    "Skipping ahead a few steps, let's take a look at an ITS_LIVE data cube so that we have some expectations about what we'll see in the data catalog and once we read a data cube into memory. \n",
    "\n",
    "Specifically, we want to understand an ITS_LIVE time series data cube in the context of the Xarray data model. If you're new to working with Xarray, the [Data Structures](https://docs.xarray.dev/en/latest/user-guide/data-structures.html) documentation is very useful for getting a hang of the different components that are the building blocks of `Xarray.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829dc0c5",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "init_url = \"http://its-live-data.s3.amazonaws.com/datacubes/v2-updated-october2024/N60W130/ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y250000.zarr\"\n",
    "datacube = xr.open_dataset(init_url, engine=\"zarr\", decode_timedelta=True, chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de315a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb514f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### {{a1_its_nb1}}\n",
    "\n",
    "#### Dimensions\n",
    "- This object has 3 *dimensions*, `mid_date`, `x`, and `y`.\n",
    "- Each dimension has a corresponding coordinate variable of the same name. Think of these as \"axes ticks\" on a figure if you were to plot the data.\n",
    "\n",
    "#### Data Variables\n",
    "- Expanding the 'Data Variables' label, you can see that there are many (60!) variables.\n",
    "- Each variable exists along one or more dimension (eg. `(mid_date,x,y)`), has an associated data type (eg.`float32`), and has an underlying array that holds that variable's data. \n",
    "\n",
    "#### Attributes\n",
    "- Data is commonly associated with related \"metadata\" -- data that describes data. For example, the `floatingice` variable has an attribute `description : floating ice mask, 0 = non-floating-ice, 1 = floating-ice` that tells you how to interpret its values. All array-based Xarray objects (data variables, coordinate variables, DataArrays and Datasets) can have attributes attached to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e16cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube.floatingice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4f996",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Other Coordinate Variables\n",
    "\n",
    "Metadata can take the form of dimensional arrays too. For example, the `satellite_img1` and `satellite_img2` arrays record the satellite sources for the image pair used to construct the velocity data. This is important *metadata* about the observed velocity fields. Such variables can be set as \"non-dimension coordinate variables\" if desired, though we will not do so here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5609f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube.satellite_img1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef2983",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{tip}\n",
    "If you haven't yet, review the {term}`Metadata naming` and {term}`Climate Forecase (CF) Metadata Conventions` sections of the [Relevant Concepts](../../background/relevant_concepts.md) page.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82771806",
   "metadata": {
    "tags": []
   },
   "source": [
    "## B. Read ITS_LIVE data from AWS S3 using Xarray\n",
    "\n",
    "Now that we know a bit more about the ITS_LIVE dataset, we can start querying the catalog to access the data we're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46458b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### {{b1_its_nb1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e3f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "The ITS_LIVE project details a number of data access options on their [website](https://its-live.jpl.nasa.gov/#access). Here, we will be accessing ITS_LIVE data in the form of [zarr](https://zarr.readthedocs.io/en/stable/) data cubes that are stored in [S3 buckets](https://registry.opendata.aws/its-live-data/) hosted by Amazon Web Services (AWS). There is a [AWS S3 explorer index](https://its-live-data.s3.amazonaws.com/index.html) that we will use to query the data catalog. There, you can browse the contents of the bucket in the AWS S3 Explorer. Click this [link](https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json) to download the file.\n",
    "\n",
    ":::{tip}\n",
    "You can also use the ITS_LIVE API to access ITS_LIVE data cube urls corresponding to different search conditions as well as Python code provided on the ITS_LIVE [website](https://its-live.jpl.nasa.gov/#api). We go through the steps of looking at the catalog in order to get a better understanding of how S3 buckets are organized. \n",
    ":::\n",
    "\n",
    "To query the data stored in the bucket, we will download the `catalog_v02.json` that is located in the bucket linked above. \n",
    "\n",
    "#### Understanding the data\n",
    "\n",
    "The first step in working with a new dataset is understanding how it is organized. To query the data stored in the bucket, we will download the `catalog_v02.json` that is linked above. This catalog contains spatial information and properties of ITS_LIVE data cubes as well as the URL used to access each cube. Let's take a look at the entry for a single data cube and the information that it contains:\n",
    "\n",
    "```{image} ../imgs/screengrab_itslive_catalog_entry.png\n",
    ":center-align\n",
    "```\n",
    "\n",
    "The top portion of the picture shows the spatial extent of the data cube in lat/lon units. Below that, we have properties such as the [EPSG code of the coordinate reference system](https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Dataset), the spatial footprint in projected units, and the url of the zarr object. \n",
    "\n",
    "Let's take a look at the url more in-depth: \n",
    "```\n",
    "http://its-live-data.s3.amazonaws.com/datacubes/v2-updated-october2024/S40E170/ITS_LIVE_vel_EPSG32759_G0120_X450000_Y5250000.zarr\n",
    "```\n",
    "\n",
    "From this link we can see that we are looking at ITS_LIVE data located in an s3 bucket hosted by Amazon Web Services (AWS). We also see that we're looking in the version 2 data cube directory. The next bit gives us information about the global location of the cube (N40E080). The actual file name `ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr` tells us that we are looking at ice velocity data (its_live also has elevation data), in the CRS associated with EPSG 32645 (this code indicates UTM zone 45N). X250000_Y4750000 tells us more about the spatial footprint of the datacube within the UTM zone. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1535a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### {{b2_its_nb1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613de52",
   "metadata": {
    "tags": []
   },
   "source": [
    "We've found the url associated with the tile we want to access, let's try to open the data cube using `Xarray.open_dataset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567cc3e",
   "metadata": {
    "tags": [
     "raises-exception",
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"http://its-live-data.s3.amazonaws.com/datacubes/v2/N30E090/ITS_LIVE_vel_EPSG32646_G0120_X750000_Y3350000.zarr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3fc72",
   "metadata": {
    "tags": []
   },
   "source": [
    "In addition to passing `url` to `xr.open_dataset()`, we include `chunks='auto'`. This introduces [dask](https://www.dask.org/) into our workflow; `chunks='auto'` will choose chunk sizes that match the underlying data structure; this is often ideal, but sometimes you may need to specify different chunking schemes. You can read more about choosing good chunk sizes [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes); subsequent notebooks in this tutorial will explore different approaches to dask chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814eb1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = xr.open_dataset(url, decode_timedelta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb59153",
   "metadata": {
    "tags": []
   },
   "source": [
    "As you can see, this doesn’t quite work. When passing the url to `xr.open_dataset()`, if a backend isn’t specified, xarray will expect a netcdf file. Because we’re trying to open a zarr file we need to add an additional argument to `xr.open_dataset()`, shown in the next code cell. You can find more information [here](https://docs.xarray.dev/en/stable/user-guide/io.html#cloud-storage-buckets).\n",
    "\n",
    "We set `decode_coords=\"all\"` so that Xarray will auto-detect a number of variables as coordinate variables --- these are variables that are usually describing properties that are common to many \"data variables\". In our case, it picks up the `mapping` variable which describes the Coordinate Reference System for this datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c159547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = xr.open_dataset(url, engine=\"zarr\", chunks=\"auto\", decode_timedelta=False, decode_coords=\"all\")\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430de7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "This one worked! Let's stop here and define a function that we can use to read additional s3 objects into memory as Xarray Datasets. This will come in handy later in this notebook and in subsequent notebooks. We will store this and other utility functions in `itslive_tools.py` for reuse across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04828f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_in_s3(http_url: str, chunks: str | dict | None = \"auto\") -> xr.Dataset:\n",
    "    \"\"\"I'm a function that takes a url pointing to the location of a zarr data cube.\n",
    "    I return an Xarray Dataset. I can take an optional chunk argument which specifies\n",
    "    how the data will be chunked when read into memory\"\"\"\n",
    "    datacube = xr.open_dataset(\n",
    "        http_url,\n",
    "        engine=\"zarr\",\n",
    "        chunks=chunks,\n",
    "        decode_coords=\"all\",\n",
    "        decode_timedelta=False,\n",
    "    )\n",
    "    return datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc28236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### {{b3_its_nb1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377860e",
   "metadata": {
    "tags": []
   },
   "source": [
    "We just read in a very large dataset. \n",
    "\n",
    "We'd like an easy way to be able to visualize the footprint of this data to ensure we specified the correct location without plotting a data variable over the entire footprint, which would be much more computationally and time-intensive. \n",
    "\n",
    "To do so, we need to understand the coordinate system of the data, and its bounds.\n",
    "\n",
    "This dataset has its coordinate system info stored in an array named `mapping`. How would you know that? Scroll through the Xarray Dataset repr, and check the attributes. Variables with CRS information tend to have the `crs_wkt`, `grid_mapping`, `GeoTransform` and related attributes that describe the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc.mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5b292",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following function creates a `GeoPandas.GeoDataFrame` describing the spatial footprint of an `xr.Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903e129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bounds_polygon(input_xr: xr.Dataset) -> gpd.GeoDataFrame:\n",
    "    \"\"\"I'm a function that takes an Xarray Dataset and returns a GeoPandas DataFrame of the bounding box of the Xarray Dataset.\"\"\"\n",
    "\n",
    "    xmin = input_xr.coords[\"x\"].data.min()\n",
    "    xmax = input_xr.coords[\"x\"].data.max()\n",
    "\n",
    "    ymin = input_xr.coords[\"y\"].data.min()\n",
    "    ymax = input_xr.coords[\"y\"].data.max()\n",
    "\n",
    "    pts_ls = [(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "    crs = f\"epsg:{input_xr.mapping.spatial_epsg}\"\n",
    "\n",
    "    polygon_geom = Polygon(pts_ls)\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])\n",
    "\n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fbfd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's take a look at the cube we've already read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b09635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox = get_bounds_polygon(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c01c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "`get_bounds_polygon()` returns a geopandas.GeoDataFrame object in the same projection as the velocity data object (local UTM). Re-project to latitude/longitude to view the object more easily on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04184c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox = bbox.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9037c",
   "metadata": {
    "tags": []
   },
   "source": [
    "To visualize the footprint, we use the interactive plotting library, [hvPlot](https://hvplot.holoviz.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880832a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly = bbox.hvplot(legend=True, alpha=0.3, tiles=\"ESRI\", color=\"red\", geo=True)\n",
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed403e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. Query ITS_LIVE catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc451d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### {{c1_its_nb1}}\n",
    "Let's look in a different region and see how we could search the ITS_LIVE data cube catalog for the granule that covers our location of interest. There are many ways to do this, this is just one example. \n",
    "\n",
    "First, we read in the catalog GeoJSON file with geopandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85b2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itslive_catalog = gpd.read_file(\"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\")\n",
    "itslive_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378266",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is a function to query the catalog for the s3 url covering a given point. You could easily tweak this function (or write your own!) to select granules based on different properties. Play around with the `itslive_catalog` object to become more familiar with the data it contains and different options for indexing.\n",
    "\n",
    ":::{note}\n",
    "Since this tutorial was originally written, the [ITS_LIVE Python Client](https://github.com/nasa-jpl/itslive-py) was released. This is a great way to access ITS_LIVE data cubes. \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c13e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_granule_by_point(input_point: list) -> str:\n",
    "    \"\"\"I take a point in [lon, lat] format and return the url of the granule containing specified point.\n",
    "    Point must be passed in EPSG:4326.\"\"\"\n",
    "\n",
    "    catalog = gpd.read_file(\"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\")\n",
    "\n",
    "    # make shapely point of input point\n",
    "    p = gpd.GeoSeries([Point(input_point[0], input_point[1])], crs=\"EPSG:4326\")\n",
    "    # make gdf of point\n",
    "    gdf = gdf = gpd.GeoDataFrame({\"label\": \"point\", \"geometry\": p})\n",
    "    # find row of granule\n",
    "    granule = catalog.sjoin(gdf, how=\"inner\")\n",
    "\n",
    "    url = granule[\"zarr_url\"].values[0]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22b36f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose a location in Alaska:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983afde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = find_granule_by_point([-138.958776, 60.748561])\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0e3d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Great, this function returned a single url corresponding to the data cube covering the point we supplied. Let's use the `read_in_s3` function we defined to open the datacube as an `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44343b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube = read_in_s3(url)\n",
    "datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf8c6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### {{c2_its_nb1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc2be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use the `get_bounds_polyon` function to take a look at the footprint using `hvplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e843b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_dc = get_bounds_polygon(datacube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd503c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly = bbox_dc.to_crs(\"EPSG:4326\").hvplot(legend=True, alpha=0.5, tiles=\"ESRI\", color=\"red\", geo=True)\n",
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09380b",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{conclusion}}  \n",
    "This notebook demonstrated how to query and access a cloud-optimized remote sensing time series dataset stored in an AWS S3 bucket. The subsequent notebooks in this tutorial will go into much more detail on how to organize, examine and analyze this data. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.188405,
   "end_time": "2025-03-22T05:49:05.536366",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/emmamarshall/Desktop/phd_research/ch1/cloud_os_geospatial_datacube_workflows/book/itslive/nbs/1_accessing_itslive_s3_data.ipynb",
   "output_path": "/home/emmamarshall/Desktop/phd_research/ch1/cloud_os_geospatial_datacube_workflows/book/itslive/nbs/1_accessing_itslive_s3_data.ipynb",
   "parameters": {},
   "start_time": "2025-03-22T05:48:51.347961",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
