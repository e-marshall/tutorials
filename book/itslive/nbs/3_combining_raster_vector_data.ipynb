{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{title_its_nb3}}\n",
    "\n",
    "{{intro}}\n",
    "\n",
    "In the previous notebook, we worked through challenges that can be associated with working with larger-than-memory datasets. Here, we'll start where we left off, with a dataset that is organized in chronological order, and has a chunking strategy applied. \n",
    "\n",
    "In this notebook, we introduce vector data that describes spatial features that we're interested in. This notebook will:\n",
    "- Look at how to parse and inspect important geographic metadata,   \n",
    "- Project data to match the coordinate referese system (CRS) of another data object,  \n",
    "- Join raster and vector data by clipping the raster data cube by the spatial extent of a vector data frame.  \n",
    "- Write this clipped raster object to disk. \n",
    "\n",
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content.Section_A)=\n",
    "**[A. Read data using strategy identified in previous notebook](#a-read-data-using-strategy-identified-in-previous-notebook)**\n",
    "\n",
    "(content.Section_B)=\n",
    "**[B. Incorporate glacier outline (vector) data](#b-incorporate-glacier-outline-vector-data)**\n",
    "- {{b1_its_nb3}}\n",
    "- {{b2_its_nb3}}\n",
    "- {{b3_its_nb3}}\n",
    "\n",
    "(content.Section_C)=\n",
    "**[C. Combine raster and vector data](#c-combine-raster-and-vector-data)**\n",
    "- {{c1_its_nb3}}\n",
    "- {{c2_its_nb3}}\n",
    ":::\n",
    ":::{tab-item} Learning Goals\n",
    "{{concepts}}\n",
    "- Understand how to parse and manage coordinate reference system metadata,  \n",
    "- Visualize vector dataframes with static and interactive plots,  \n",
    "- Spatial subsetting and clipping with vector and raster data,\n",
    "- Write raster data to disk.\n",
    "\n",
    "\n",
    "{{techniques}}\n",
    "- Use [cf_xarray](https://cf-xarray.readthedocs.io/en/latest/) and [pyproj](https://pyproj4.github.io/pyproj/stable/) to access geographic metadata,\n",
    "- Perform spatial subset of vector dataframe with [GeoPandas](https://geopandas.org/en/stable/),\n",
    "- Clip raster data using vector data with [rioxarray](https://corteva.github.io/rioxarray/).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import cf_xarray\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import pyproj\n",
    "\n",
    "import itslive_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read data using strategy identified in previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be reading and writing files to disk, create a variable to hold the path to the root directory for this tutorial; we'll use this later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial1_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find url\n",
    "url = itslive_tools.find_granule_by_point([95.180191, 30.645973])\n",
    "# Read data cube without Dask\n",
    "dc = itslive_tools.read_in_s3(url, chunks=None)\n",
    "# Sort by mid-date\n",
    "dc = dc.sortby(\"mid_date\")\n",
    "# Visually check mid-date in chronological order\n",
    "dc.mid_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the last notebook, we want to chunk the dataset and to do that, we need to assign chunk sizes for each dimension. I copied the dictionary below from `dc_auto.chunksizes` in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "chunking_dict = {\n",
    "    \"mid_date\": (32736, 15156),\n",
    "    \"y\": (\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        13,\n",
    "    ),\n",
    "    \"x\": (\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        13,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.chunk(chunking_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Incorporate glacier outline (vector) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{b1_its_nb3}}\n",
    "\n",
    "As discussed in the [Software and Data](software_and_data.ipynb#RGI) notebook, the examples in this tutorial use glacier outlines from the Randolph Glacier Inventory, version 7 (RGI7). We'll specifically be looking at the 'South Asia East' region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_parquet(\"../data/rgi7_region15_south_asia_east.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is **vital** to check the CRS, or Coordinate Reference Systems, when combining geospatial data from different sources. \n",
    "\n",
    "The RGI data are in the `EPSG:4326` CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRS information for the ITS_LIVE dataset is stored in the `mapping` array. An easy way to discover this is to use the `cf_xarray` package and search for the `grid_mapping` variable if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_crs = pyproj.CRS.from_cf(dc.mapping.attrs)\n",
    "cube_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the data is projected to UTM zone 46N (EPSG:32646).\n",
    "\n",
    "We choose to reproject the glacier outline to the CRS of the datacube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project rgi data to match itslive\n",
    "se_asia_prj = se_asia.to_crs(cube_crs)\n",
    "se_asia_prj.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many glaciers are represented in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(se_asia_prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{b2_its_nb3}}\n",
    "\n",
    "In [Accessing S3 Data](accessing_s3_data.ipynb), we defined a function to create a vector object describing the footprint of a raster object; we'll use that again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector bbox of itslive\n",
    "bbox_dc = itslive_tools.get_bounds_polygon(dc)\n",
    "bbox_dc[\"geometry\"]\n",
    "# Check that all objects have correct crs\n",
    "assert dc.attrs[\"projection\"] == bbox_dc.crs == se_asia_prj.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outline of the itslive granule and the rgi dataframe together\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bbox_dc.plot(ax=ax, facecolor=\"None\", color=\"red\")\n",
    "se_asia_prj.plot(ax=ax, facecolor=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{b3_its_nb3}}\n",
    "\n",
    "The above plot shows the coverage of the vector dataset (`se_asia_prj`) in black, relative to the extent of the raster dataset (`bbox_dc`) in red. We use the [geopandas `.clip()`](https://geopandas.org/en/stable/docs/reference/api/geopandas.clip.html) method to subset the RGI polygons  to the footprint of the ITS_LIVE datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rgi to bounds\n",
    "se_asia_subset = gpd.clip(se_asia_prj, bbox_dc)\n",
    "se_asia_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `geopandas` `.explore()` method to interactively look at the RGI7 outlines contained within the ITS_LIVE granule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    max_lat=31, max_lon=95, min_lat=29, min_lon=97, location=[30.2, 95.5], zoom_start=8\n",
    ")\n",
    "\n",
    "bbox_dc.explore(\n",
    "    m=m,\n",
    "    style_kwds={\"fillColor\": \"None\", \"color\": \"red\"},\n",
    "    legend_kwds={\"labels\": [\"ITS_LIVE granule footprint\"]},\n",
    ")\n",
    "se_asia_subset.explore(m=m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above interactive map to select a glacier to look at in more detail below.\n",
    "\n",
    "However, notice that while the above code correctly produces a plot, it also throws a warning. We're going to ignore the warning for now, but if you're interested in a detailed example of how to trouble shoot and resolve this type of warning, check out the [appendix](../pt4/appendix.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write `bbox_dc` to file so that we can use it in the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_dc.to_file(\"../data/bbox_dc.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Combine raster and vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{c1_its_nb3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to dig in and analyze this velocity dataset at smaller spatial scales, we first need to subset it. The following section and the next notebook ([Exploratory data analysis of a single glacier](4_exploratory_data_analysis_single.ipynb)) will focus on the spatial scale of a single glacier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a  glacier to subset to\n",
    "single_glacier_vec = se_asia_subset.loc[\n",
    "    se_asia_subset[\"rgi_id\"] == \"RGI2000-v7.0-G-15-16257\"\n",
    "]\n",
    "single_glacier_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it to file to that it can be used later\n",
    "single_glacier_vec.to_file(\"../data/single_glacier_vec.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the ITS_LIVE raster dataset has an assigned CRS attribute. We already know that the data is projected in the correct coordinate reference system (CRS), but the object may not be 'CRS-aware' yet (ie. have an attribute specifying its CRS). This is necessary for spatial operations such as clipping and reprojection. If `dc` doesn't have a CRS attribute, use `rio.write_crs()` to assign it. For more detail, see Rioxarray's [CRS Management documentation](https://corteva.github.io/rioxarray/stable/getting_started/crs_management.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.rio.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the subset vector data object and Rioxarray's [`.clip()` method](https://corteva.github.io/rioxarray/html/examples/clip_geom.html) to crop the data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "single_glacier_raster = dc.rio.clip(single_glacier_vec.geometry, single_glacier_vec.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {{c2_its_nb3}}\n",
    " \n",
    "We want to use `single_glacier_raster` in the following notebook without going through all of the steps of creating it again. So, we write the object to file as a Zarr data cube so that we can easily read it into memory when we need it next. However, we'll see that there are a few steps we must go through before we can successfully write this object. \n",
    "\n",
    "\n",
    "We first re-chunk the `single_glacier_raster` into more optimal chunk sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster = single_glacier_raster.chunk(\n",
    "    {\"mid_date\": 20000, \"x\": 10, \"y\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to write `single_glacier_raster`, eg: \n",
    "\n",
    "```single_glacier_raster.to_zarr('data/itslive/glacier_itslive.zarr', mode='w')```\n",
    "\n",
    "We'll received an error related to encoding. \n",
    "\n",
    "The root cause is that the encoding recorded was appropriate for the source dataset, but is not valid anymore given all the transformations we have run up to this point. The easy solution here is to simply call `drop_encoding`. This will delete any existing encoding isntructions, and have Xarray automatically choose an encoding that will work well for the data. Optimizing the encoding of on-disk data is an advanced topic that we will not cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster = single_glacier_raster.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster = single_glacier_raster.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "2/26 running this i got a ClientResponseError:500 when trying to write to zarr. Added in above two steps and it worked, not entirely sure why? if keep these in, add explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_glacier_raster.drop_encoding().to_zarr(\n",
    "    \"../data/single_glacier_itslive.zarr\", mode=\"w\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to write the object as a Zarr group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{conclusion}}\n",
    "\n",
    "In this notebook, we read a large object into memory and clipped it to the footprint of a single area of using a large vector dataframe, all the while managing coordinate reference system metadata of both objects. We then saved the clipped raster object to disk so that it can be easily re-used. The next notebook demonstrates exploratory data analysis steps using the object we just wrote to disk. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
